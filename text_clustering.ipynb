{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mission Statement\n",
    "* In this simulation, a popular YouTuber has asked to analyze the comment section from one of his recent videos\n",
    "* Asks:\n",
    "    * What are the broad things people talking about?\n",
    "    * He has ads in the video. Are people talking about his ads?\n",
    "    * Anything else useful we might find\n",
    "\n",
    "General Directions from the YouTuber:\n",
    "* We want the number of likes on a comment to inform the process somehow. Likes is big part of engagement.\n",
    "* There are obviously lots and lots of things in the comments section, but we are to somehow make sense of them and report back"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Process\n",
    "* Pull the data from Youtube\n",
    "    * Get all comments and the number of likes for each comment\n",
    "* Clean the data\n",
    "    * Remove any duplicates\n",
    "        * The pinned comment is the only a true duplicate\n",
    "        * If the same comment is posted by different users, combine it into one comment with the total number of likes\n",
    "    * Translate all comments to English\n",
    "    * We can assume any comments with zero likes is just noise\n",
    "* Get text embedding of each unique comment\n",
    "* Perform dimensionality reduction\n",
    "    * Clustering algorithms generally perform better in relatively low dimensional space\n",
    "* Cluster the comments\n",
    "    * Use a weighted clustering clustering algorithm with the number of likes acting as the sample weight\n",
    "* Perform topicl modeling (TF-IDF) to determine the topic of each cluster\n",
    "* Depending on the number of clusters/topics, combine the clusters\n",
    "* Report the broad topics\n",
    "* Report about engagement with the ads\n",
    "* Report anything else that's useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.translation.text import TextTranslationClient\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyyoutube import Api\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from umap import UMAP\n",
    "\n",
    "# APIs\n",
    "GOOGLE_API_KEY = os.environ.get(\"GOOGLE_KEY\")\n",
    "AZURE_KEY = os.environ.get(\"AZURE_KEY\")\n",
    "AZURE_ENDPOINT = \"https://api.cognitive.microsofttranslator.com\"\n",
    "AZURE_REGION = \"southcentralus\"\n",
    "\n",
    "# Video ID\n",
    "VIDEO_ID = \"KOEfDvr4DcQ\"\n",
    "\n",
    "# Save the intermediate steps so we don't have to do reprocessing\n",
    "CACHE_DIR = \"./data/youtube_comments\"\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "RAW_COMMENTS_PATH        = os.path.join(CACHE_DIR, VIDEO_ID + \"_raw.csv\")\n",
    "CLEANED_COMMENTS_PATH    = os.path.join(CACHE_DIR, VIDEO_ID + \"_cleaned.csv\")\n",
    "TRANSLATED_COMMENTS_PATH = os.path.join(CACHE_DIR, VIDEO_ID + \"_translated.csv\")\n",
    "FINAL_COMMENTS_PATH      = os.path.join(CACHE_DIR, VIDEO_ID + \"_final.csv\")\n",
    "EMBEDDINGS_PATH          = os.path.join(CACHE_DIR, VIDEO_ID + \"_embeddings.npy\")\n",
    "FIRST_CLUSTERING_PATH    = os.path.join(CACHE_DIR, VIDEO_ID + \"_first_clustering.csv\")\n",
    "\n",
    "# Model IDs\n",
    "EMBEDDING_MODEL_ID = \"Alibaba-NLP/gte-Qwen2-7B-instruct\"\n",
    "EMBEDDING_MAX_LENGTH = 8192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect the data\n",
    "# The text to be clustered are all comments on Mr Beast's most watched video of 2024\n",
    "#   His most watched video of 2024 as of 02/2025 is \"Face Your Biggest Fear To Win $800,000\"\n",
    "# At time of data collection, the video has ~300M views and ~215k comments\n",
    "\n",
    "# If the comments have already been pulled and saved, just load them\n",
    "if os.path.exists(RAW_COMMENTS_PATH):\n",
    "    raw_comments = pd.read_csv(RAW_COMMENTS_PATH)\n",
    "    raw_comments[\"Text\"] = raw_comments[\"Text\"].fillna(\"\")\n",
    "\n",
    "# If they haven't been saved, pull them then save them\n",
    "else:\n",
    "    responses = []\n",
    "    token = None\n",
    "\n",
    "    # Get a batch of 1000 comment threads\n",
    "    # response is a linked list. If response.nextPageToken is not None, there are more comments\n",
    "    # Get responses until there are no more (response.nextPageToken is None)\n",
    "    api = Api(api_key=GOOGLE_API_KEY)\n",
    "    while True:\n",
    "        response = api.get_comment_threads(\n",
    "            video_id=VIDEO_ID,\n",
    "            count=1000,\n",
    "            page_token=token,\n",
    "            text_format=\"plainText\"\n",
    "        )\n",
    "\n",
    "        # Append and get the next page\n",
    "        responses.append(response)\n",
    "        token = response.nextPageToken\n",
    "\n",
    "        # If this is the last page, break\n",
    "        if not token:\n",
    "            break\n",
    "    \n",
    "    # Get the text, number of likes, and ID of each comment. Save it to a csv\n",
    "    raw_comments = [\n",
    "        (thread.snippet.topLevelComment.snippet.textDisplay, thread.snippet.topLevelComment.snippet.likeCount, thread.id)\n",
    "        for response in responses for thread in response.items\n",
    "    ]\n",
    "    raw_comments = pd.DataFrame(raw_comments, columns=[\"Text\", \"Likes\", \"Id\"])\n",
    "    raw_comments.to_csv(RAW_COMMENTS_PATH, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "* Of the ~215k comments, there are only ~144k unique comment texts (many repeated comments)\n",
    "    * This seems to be mostly a combination of bots reposting comments, and trivial messages (many times just an emoji)\n",
    "    * However, it is also the same (non-trival) comment being naturally posted several times\n",
    "* We can combine two rows if their text is the same, and add their likes together\n",
    "* Of the 144k unique comments, only ~33k have at least 1 like\n",
    "    * We can assume the unliked comments are noise, so we can remove them\n",
    "* Since we will be doing topic modeling with TF-IDF we want all comments to be in English\n",
    "    * To translate, we can just run all the comments through Microsoft Cognitive Services Text Translation\n",
    "    * Translation is generally good, but isn't perfect\n",
    "        * Short comments in English sometimes get labeled as some other language then get a \"wonky\" translation back to English. Ex:\n",
    "            * Original: &emsp;*Maaacck!!...MUAAAACCK* !! - Arnold Schwarzenegger (Tahitian detected)\n",
    "            * Translated: *Maaacck!! ... MUAAAACCK* - Arnold Arnold Arnold\n",
    "    * ~70% of unique comment texts are detected as English\n",
    "        * ~97% of likes are on comments detected as English\n",
    "* After translating, there are duplicates. We can combine them just as before\n",
    "    * \"Hola\" and \"Hello\" wouldn't originally be combined, but after translating both now say \"Hello\" and can be combined\n",
    "    * We can combine an additional ~1.8% of the ~33k comments after translating (reduces 33.2k comments down to 32.6k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicated and comments with 0 likes\n",
    "if os.path.exists(CLEANED_COMMENTS_PATH):\n",
    "    cleaned_comments = pd.read_csv(CLEANED_COMMENTS_PATH)\n",
    "    cleaned_comments[\"Text\"] = cleaned_comments[\"Text\"].fillna(\"\")\n",
    "\n",
    "else:\n",
    "    # Mr Beast's pinned comment is the only true duplicated comment\n",
    "    cleaned_comments = raw_comments.drop_duplicates()\n",
    "\n",
    "    # Combine two or more comments if their text is identical. Sum the likes from all combined comments\n",
    "    #   Keep the ID of the comment with more likes\n",
    "    cleaned_comments = cleaned_comments.sort_values(by=\"Likes\", ascending=False)\n",
    "    cleaned_comments = cleaned_comments.groupby(\"Text\", as_index=False).agg({\n",
    "        \"Likes\": \"sum\",\n",
    "        \"Id\": \"first\"\n",
    "    })\n",
    "\n",
    "    # Most comments still have 0 likes. Anything with zero noise is considered noise\n",
    "    cleaned_comments = cleaned_comments[cleaned_comments[\"Likes\"] > 0]\n",
    "\n",
    "    # Save the cleaned comments to disk\n",
    "    cleaned_comments.to_csv(CLEANED_COMMENTS_PATH, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate all comments to English\n",
    "#   We need all comments in English so we can properly perform TF-IDF later\n",
    "#   We want to translate before embedding since language is likely represented in the embedding\n",
    "#       Unfortunately, the translation isn't perfect, but it will be good enough\n",
    "# We translate after combining so we don't translate the same exact piece of text multiple times\n",
    "\n",
    "# Free tier of translation service has a rate limit of 33k characters per minute. The documentation only mentions a 2M\n",
    "#   limit per hour, but if I send more than 33k characters per minute (2M per hour average) I get an \"exceeded request\n",
    "#   limits\" error.\n",
    "# To make the code as simple as possible since it only needs to run once, I found that batching the requests into 200\n",
    "#   comments never exceeds 33k characters. I can then wait a minute between each batch to avoid the rate limit.\n",
    "# If this code was going to be run repeatedly, I'd use a variable batch size and pack each batch based on the character\n",
    "#   count of each comment in the batch.\n",
    "def language_and_enlish_translation(texts, batch_size = 200):\n",
    "    \"\"\" Given a list of texts returns their source language and the translation of each item to English \"\"\"\n",
    "    client = TextTranslationClient(credential=AzureKeyCredential(AZURE_KEY), region=AZURE_REGION, endpoint=AZURE_ENDPOINT)\n",
    "    response = []\n",
    "\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        texts_batch = texts[i:i + batch_size]\n",
    "        response += client.translate(body=texts_batch, to_language=[\"en\"])\n",
    "        time.sleep(60)\n",
    "\n",
    "    return [\n",
    "        (item[\"detectedLanguage\"][\"language\"], item[\"translations\"][0][\"text\"])\n",
    "        for item in response\n",
    "    ]\n",
    "\n",
    "if os.path.exists(TRANSLATED_COMMENTS_PATH):\n",
    "    translated_comments = pd.read_csv(TRANSLATED_COMMENTS_PATH)\n",
    "    translated_comments[[\"Text\", \"English Text\"]] = translated_comments[[\"Text\", \"English Text\"]].fillna(\"\")\n",
    "\n",
    "else:\n",
    "    translated_comments = cleaned_comments.copy()\n",
    "    translated_comments.loc[:, [\"Language\", \"English Text\"]] = language_and_enlish_translation(translated_comments[\"Text\"].tolist())\n",
    "    translated_comments.to_csv(TRANSLATED_COMMENTS_PATH, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine again after translating\n",
    "# Now that everything is English, there may be more repeated comments\n",
    "#   If the same comment was in two different languages, by translating all to English they would now be redundant\n",
    "#   Previously \"Hola\" and \"Hello\" would not be combined. After translating both would be \"Hello\" and would be combined\n",
    "\n",
    "if os.path.exists(FINAL_COMMENTS_PATH):\n",
    "    final_comments = pd.read_csv(FINAL_COMMENTS_PATH)\n",
    "    final_comments[[\"Text\", \"English Text\"]] = final_comments[[\"Text\", \"English Text\"]].fillna(\"\")\n",
    "\n",
    "else:\n",
    "    # Combine if the English Text is the same and sum the likes\n",
    "    final_comments = translated_comments.copy()\n",
    "    final_comments = final_comments.sort_values(by=\"Likes\", ascending=False)\n",
    "    final_comments = final_comments.groupby(\"English Text\", as_index=False).agg({\n",
    "        \"Likes\": \"sum\",\n",
    "        \"Text\": \"first\",\n",
    "        \"Language\": \"first\",\n",
    "        \"Id\": \"first\"\n",
    "    })\n",
    "\n",
    "    # Reset the index after cleaning is complete\n",
    "    final_comments = final_comments.reset_index(drop=True)\n",
    "    \n",
    "    # Save the final comments to disk\n",
    "    final_comments.to_csv(FINAL_COMMENTS_PATH, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print the Top 10 Comments\n",
    "Preliminary Analysis\n",
    "* All the top comments are in English\n",
    "* The most liked comment is Mr Beast's pinned comment\n",
    "* The number of likes looks like it follows an exponential decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print top 10 most liked comments\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "pd.set_option(\"display.width\", 1000)\n",
    "final_comments[[\"English Text\", \"Likes\", \"Language\"]].nlargest(10, \"Likes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pie Chart of Distribution of Languages of Unique Comments with 1 or more Likes\n",
    "* We can view the distribution of languages for unique comment texts with 1+ like\n",
    "    * We combined comments that had identical texts\n",
    "    * We removed comments that had zero likes\n",
    "* This pie chart does show us that it was probably a good idea to translate all comments, even if some of the translation weren't perfect\n",
    "    * Only 70% of unique comments with 1+ likes were in English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pie Chart of Distribution of Languages of Unique Comments with 1 or more Likes\n",
    "\n",
    "# Top Languages to show\n",
    "language_map = {\n",
    "    \"en\": \"English\",\n",
    "    \"es\": \"Spanish\",\n",
    "    \"ru\": \"Russian\",\n",
    "    \"so\": \"Somali (Arabic)\",\n",
    "    \"ar\": \"Arabic\",\n",
    "    \"pt\": \"Portuguese (Brazil)\",\n",
    "}\n",
    "\n",
    "# Get the count per language. Combine less common languages into \"Other\"\n",
    "# Note:\n",
    "#   This is the count of unique comments with 1+ likes\n",
    "#   We already removed duplicate texts and comments wtih 0 likes\n",
    "language_counts = translated_comments['Language'].replace(language_map).value_counts()\n",
    "top_languages = language_counts.nlargest(len(language_map))\n",
    "other_count = language_counts.iloc[len(language_map):].sum()\n",
    "top_languages['Other'] = other_count\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "explode = [0] + [0.1]*len(language_map)\n",
    "top_languages.plot.pie(\n",
    "    ax=ax,\n",
    "    startangle=54.36,\n",
    "    explode=explode,\n",
    "    labeldistance=1.03,\n",
    "    autopct=\"%.1f%%\",\n",
    "    pctdistance=0.80\n",
    ")\n",
    "ax.set_ylabel('')\n",
    "fig.suptitle(\"Distribution of Languages\\nof Unique Comments with 1+ Likes\", x=0.55, y=0.95)\n",
    "ax.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pie Chart of Total Likes per Language\n",
    "* We can view the distribution of likes for each language\n",
    "    * Even with all the previous filtering, the likes per language will be preserved\n",
    "* ~97% of all likes are on English comments\n",
    "    * This isn't totally surprising since the primary language of the video is English\n",
    "    * This means that even if translation from other languages to English aren't perfect, it is a small amount of the mass anyway\n",
    "    * This could mean that the \"wonky\" translationg are a larger problem than just using the raw text, even if it is another language\n",
    "        * A \"wonky\" translation is when an English comment get labeled as some other language then is incorrected translated to English. Ex:\n",
    "            * Original: &emsp;*Maaacck!!...MUAAAACCK* !! - Arnold Schwarzenegger (Tahitian detected)\n",
    "            * Translated: *Maaacck!! ... MUAAAACCK* - Arnold Arnold Arnold\n",
    "* The language with the 2nd most likes was Spanish with 1.1%\n",
    "* The language with the 3rd most likes was Russian with 0.5%\n",
    "    * Adding Russian to the chart begins to make it unreadable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pie Chart of Total Likes per Language\n",
    "language_map = {\n",
    "    \"en\": \"English\",\n",
    "    \"es\": \"Spanish\",\n",
    "}\n",
    "\n",
    "# Get the count of likes per language. Combine less common languages into \"Other\"\n",
    "# Note:\n",
    "#   We preserved likes when removing duplicate texts and only removed comments with 0 likes\n",
    "#   So the count of likes per language is the same here as it would be before any data filtering\n",
    "likes_per_language = translated_comments.groupby('Language')['Likes'].sum()\n",
    "likes_per_language = likes_per_language.sort_values(ascending=False)\n",
    "likes_per_language = likes_per_language.rename(index=language_map)\n",
    "top_likes = likes_per_language.nlargest(len(language_map))\n",
    "other_likes = likes_per_language.iloc[len(language_map):].sum()\n",
    "top_likes['Other'] = other_likes\n",
    "\n",
    "# Plot the pie chart\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "explode = [0] + [0.1]*len(language_map)\n",
    "top_likes.plot.pie(\n",
    "    ax=ax,\n",
    "    startangle=5.6,\n",
    "    labeldistance=1.03,\n",
    "    explode=explode,\n",
    "    autopct=\"%.1f%%\",\n",
    "    pctdistance=0.80\n",
    ")\n",
    "ax.set_ylabel('')\n",
    "fig.suptitle(\"Total Likes per Language\", x=0.525, y=0.90)\n",
    "ax.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histogram of Tokenized Lengths\n",
    "* We can view the distribution of text lengths based on word count, character count, and tokenized length\n",
    "* Most comments are shorter than 20 words. Most are less than 111 characters. Most are less than 29 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograms of Lengths\n",
    "\n",
    "# Calculate text statistics\n",
    "word_counts = final_comments[\"English Text\"].apply(lambda x: len(x.split()))\n",
    "char_counts = final_comments[\"English Text\"].apply(len)\n",
    "tokenizer = AutoTokenizer.from_pretrained(EMBEDDING_MODEL_ID)\n",
    "token_lengths = [len(tokenizer.encode(comment)) for comment in final_comments[\"English Text\"]]\n",
    "\n",
    "# Create subplots with shared y-axis\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5), sharey=True)\n",
    "num_bins = 101\n",
    "tick_step = 10\n",
    "\n",
    "# Word Count Histogram\n",
    "axes[0].hist(word_counts, bins=np.arange(num_bins) - 0.5, density=True)\n",
    "axes[0].set_title(\"Word Count Distribution\")\n",
    "axes[0].set_xlabel(\"Number of Words\")\n",
    "axes[0].set_ylabel(\"Density\")\n",
    "axes[0].set_xticks(np.arange(0, num_bins + 1, tick_step))\n",
    "\n",
    "# Character Count Histogram\n",
    "axes[1].hist(char_counts, bins=np.arange(num_bins) - 0.5, density=True)\n",
    "axes[1].set_title(\"Character Count Distribution\")\n",
    "axes[1].set_xlabel(\"Number of Characters\")\n",
    "axes[1].set_xticks(np.arange(0, num_bins + 1, tick_step))\n",
    "\n",
    "# Tokenized Length Histogram\n",
    "axes[2].hist(token_lengths, bins=np.arange(num_bins) - 0.5, density=True)\n",
    "axes[2].set_title(\"Tokenized Length Distribution\")\n",
    "axes[2].set_xlabel(\"Number of Tokens\")\n",
    "axes[2].set_xticks(np.arange(0, num_bins + 1, tick_step))\n",
    "\n",
    "# Adjust layout and show plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"95% of comments have less than {np.percentile(word_counts, 95)} words\")\n",
    "print(f\"95% of comments have less than {np.percentile(char_counts, 95)} characters\")\n",
    "print(f\"95% of comments have less than {np.percentile(token_lengths, 95)} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histogram of Number of Likes\n",
    "* There are ~1.1M likes between all 33k comments.\n",
    "* Looking at the histogram, we see most of the comments have only a few likes\n",
    "    * The most liked 100 comments are omitted from the histogram to make it more readable\n",
    "    * The comment with the most likes is Mr Beast's pinned comment with ~114k likes\n",
    "        * There are only 10 comments with more than 10k likes. These 10 comments have ~330k likes total (31% of all comment likes)\n",
    "        * There are only 25 comments with more than 5k likes. These 25 comments have ~430k likes total (40% of all comment likes)\n",
    "        * There are only 100 comments with more than 1600 likes. These 100 comments have ~630k likes total (59% of all comment likes)\n",
    "* Note: We removed all comments with 0 likes\n",
    "* Note: We combined 2 comments if they had identical texts\n",
    "    * 2 comments with 15 likes would appear on the histogram as 1 comment with 30 likes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of Number of likes\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(final_comments[\"Likes\"], bins=np.arange(0, 1600, 16), log=True)\n",
    "plt.xlabel(\"Number of Likes\")\n",
    "plt.ylabel(\"Frequency (Log Scale)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cumulative Distribution of Likes\n",
    "* An alternative to the histogram of number of likes is the cumulative distribution of likes\n",
    "* Looking at the cumulative sum of likes vs numbner of top comments\n",
    "    * The top 1 comment has ~11% of all comment likes\n",
    "    * The top 1% of comments have 77% of all comment likes\n",
    "    * The top 2% of comments have 86% of all comment likes\n",
    "    * The top 5% of comments have 93% of all comment likes\n",
    "    * The top 10% of comments have 96% of all comment likes\n",
    "* Note: We removed all comments with 0 likes\n",
    "* Note: We combined 2 comments if they had identical texts\n",
    "    * 2 comments with 15 likes would appear on the histogram as 1 comment with 30 likes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cumulative Distribution of Likes\n",
    "sorted_likes = final_comments[\"Likes\"].sort_values(ascending=False).to_numpy()\n",
    "cumulative_likes = np.cumsum(sorted_likes) / sum(sorted_likes)\n",
    "cumulative_likes = np.insert(cumulative_likes, 0, 0.0) * 100\n",
    "\n",
    "plt.plot(np.linspace(0, 100, len(cumulative_likes), endpoint=True), cumulative_likes)\n",
    "plt.grid()\n",
    "plt.xlabel(\"Percentage of Top Comments Considered\")\n",
    "plt.ylabel(\"Cumulative Percentage of Likes\")\n",
    "plt.title(\"Cumulative Distribution of Likes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding\n",
    "* In order to find common topics, we need to convert the raw text into embeddings\n",
    "    * An embedding is just a vector. The goal is that the vector captures semantic similarities so comments with similar meanings are close together even if the actual words used are very different\n",
    "* To get an embedding, we use an embedding model\n",
    "    * It is just a model that takes in text (or tokenized text) and outputs an embedding\n",
    "    * This model is not trained on our data, but we hope the data it was trained on is similar enough that it will  group similar comments near each other\n",
    "* Researchers at Hugging Face published a set of benchmark tasks for embedding models, \"MTEB: Massive Text Embedding Benchmark\"\n",
    "    * There are just over 100 tasks divided between a number of tasks\n",
    "* MTEB has a clustering task, and has several datasets for clustering\n",
    "    * Unfortunately, the only \"web domain\" data is from Stack Exchange which is not exactly apples-to-apples with our data but is likely close enough\n",
    "    * There is other data from the internet, but sources like Wikipedia or simlar are considered encyclopaedic, non-fiction, or similar. Wikipedia and such datasets would not be similar to YouTube comments. Stack Exchange is the only \"web\" data.\n",
    "* Hugging Face has a leaderboards of all model submissions and allows us to rank by certain tasks and datasets\n",
    "    * The top 2 performing models on the clustering task with Stack Exchange data (as of the time of writing this) are gte_Qwen1.5-7B-instruct and gte-Qwen2-7B-instruct with scores of 80.60 and 80.26 respectively\n",
    "    * The scores are likely close enough that the Stack Exchange Clustering alone could not definitively tell us which model would be better on our dataset of YouTube comments\n",
    "    * Given that, I have chosen to use the Qwen2 model as it generally has better performance than the Qwen1.5 model\n",
    "* Note: The embeddings are normalized by the Qwen2 model to have length 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(EMBEDDINGS_PATH):\n",
    "    embeddings = np.load(EMBEDDINGS_PATH)\n",
    "else:\n",
    "    model = SentenceTransformer(EMBEDDING_MODEL_ID, trust_remote_code=True)\n",
    "    model.max_seq_length = EMBEDDING_MAX_LENGTH\n",
    "    embeddings = model.encode(final_comments[\"English Text\"].tolist(), batch_size=1, show_progress_bar=True)\n",
    "    np.save(EMBEDDINGS_PATH, embeddings)\n",
    "    del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction\n",
    "* Clustering algorithms, and especially density based clustering algorithms, generally don't work well is very high dimensional spaces\n",
    "    * The embedding model outputs a 3584 dimensional vector for each comment\n",
    "* PCA is a linear method for dimensionality reduction\n",
    "    * We won't actually be using PCA for dimensionality reduction, but PCA can give us some insight into the trade-off of dimensionality reduction vs information loss\n",
    "        * Explained variance approximates \"information loss\" in this case\n",
    "    * The cumulative distribution of explained variance vs number of dimensions shows us that the first few dimensions can explain most of the variance\n",
    "        * A 36 dimensional represenation of the original embeddings has 1% the number of dimensions but can explain 55% of variance\n",
    "    * We can preserve 80% of the explained variance with 217 dimensions\n",
    "        * For 90%, we would need to jump all the way up to 538 dimensions\n",
    "* UMAP is a non-linear method for dimensionality reduction that claims to preserve both local and global structures in the data\n",
    "    * UMAP is often used for clustering because it has nice properties when given complex data\n",
    "        * Generally preserves local structure and projects similar points near each other\n",
    "        * Generally preserves global structure, so different clusters are well seperated\n",
    "        * Can have noise reducing properties\n",
    "    * UMAP isn't perfect, and there are valid critisims of it\n",
    "    * UMAP will be used as the dimensionality reduction method for this demo\n",
    "* UMAP requires n_neighbors, n_components, and min_dist as parameters\n",
    "    * n_neighbors determines the balance between local vs global features\n",
    "        * This analysis is looking for broad, main themes\n",
    "        * I use a value of 30 which should balance both local and global structure, but possbily leaning more towards global structure\n",
    "    * n_components is the dimensional space of after reduction\n",
    "        * I use 217 which is an informed guess from PCA\n",
    "    * min_dist is the closest distance that points can be packed together in the new space\n",
    "        * Since we want high density clusters, we set min_dist to 0.0 to allow UMAP to pack the points tightly\n",
    "* Since we are still using a relatively high dimensional space, I use the cosine metric instead of Euclidean or others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cumulative explained variance from PCA\n",
    "pca = PCA()\n",
    "pca.fit(embeddings)\n",
    "explained_variance_first = np.cumsum(pca.explained_variance_ratio_)\n",
    "explained_variance_first = np.insert(explained_variance_first, 0, 0.0)\n",
    "\n",
    "# Plot cumulative explained variance\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(0, len(explained_variance_first)), explained_variance_first)\n",
    "plt.xlabel(\"Number of Dimensions\")\n",
    "plt.ylabel(\"Cumulative Explained Variance\")\n",
    "plt.title(\"Cumulative Distribution of Explained Variance in PCA\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter - Choose the minimum number of dimensions that explains 80% of the variance\n",
    "EXPLAINED_VARIANCE_FIRST = 0.8\n",
    "n_components_first = np.argmax(explained_variance_first >= EXPLAINED_VARIANCE_FIRST)\n",
    "\n",
    "# Hyperparameter - Min Samples is twice the number of dimensions\n",
    "MIN_SAMPLES_FIRST = 2 * n_components_first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want the number of dimensions that explains 80% of the explained variance (217 dims)\n",
    "# min_dist is 0.0 so get points clustered as closely together as possible\n",
    "# n_neighbors of 30 balances local and global structures\n",
    "# 217 is still a high dimensional embedding, so we use cosine for our metric\n",
    "# use a random state for repeatability, and n_jobs=1 because the UMAP parallelism doesn't work when seeded\n",
    "umap_model = UMAP(\n",
    "    n_components=n_components_first,\n",
    "    min_dist=0.0,\n",
    "    n_neighbors=30,\n",
    "    metric='cosine',\n",
    "    random_state=42,\n",
    "    n_jobs=1 # UMAP does not allow parallelism when seeded\n",
    ")\n",
    "dim_reduced_embeddings = umap_model.fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering\n",
    "* We will use DBSCAN for clustering to find regions with high density\n",
    "* DBSCAN has two parameters: eps and min_samples\n",
    "    * To determine eps, I use the Kneedle Method (Finding a \"Kneedle in a Haystack: Detecting Knee Points in System Behavior)\n",
    "        * The Kneedle Method is uses the idea that we can get a gauge on the value of eps by looking at the distance between points in our embedding space\n",
    "        * Each \"distance\" is the distance from one point to its K-th nearest neighbor\n",
    "            * K can be taken to be value used for min_samples (2 * dimensionality of embedding)\n",
    "        * We can sort the distances and plot them with the x-value being the sorted index and the y-value being the distance\n",
    "        * There should be an \"elbow\" or a \"knee\" in the plot\n",
    "            * This method is a subset of \"Elbow Methods\"\n",
    "        * Kneedle Algorithm\n",
    "            * The Kneedle method begins by plotting the line from the start of the curve to the end\n",
    "            * The knee is then the point on the curve that is farthest from that line\n",
    "        * This is obvisouly not going to be optimal in any way, but we have to make some kind of educated guess\n",
    "    * For min_samples, it is often useful to use domain knowledge to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_intra_cluster_variance(cluster_embeddings):\n",
    "    \"\"\" Calculates the intra-cluster variance for a given cluster \"\"\"\n",
    "    centroid = np.mean(cluster_embeddings, axis=0)\n",
    "    variance = np.sum((cluster_embeddings - centroid) ** 2)\n",
    "    return variance / len(cluster_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kneedle Method\n",
    "def kneedle(n_neighbors, embeddings, plot=False):\n",
    "    # For each point, get the distance to its K-th nearet neighbor\n",
    "    nbrs = NearestNeighbors(n_neighbors=n_neighbors, metric=\"cosine\").fit(embeddings)\n",
    "    distances, __ = nbrs.kneighbors(embeddings)\n",
    "    dist_sorted = sorted(distances[:, distances.shape[1] - 1], reverse=True)\n",
    "\n",
    "    # Consider the distances curve\n",
    "    # Find the distance from the distances curve to the straight line that connects the start and end of that curve\n",
    "    #   This is the difference curve\n",
    "    endpoint_line = np.linspace(dist_sorted[0], 0, len(distances), endpoint=True)\n",
    "    dist_difference = endpoint_line - dist_sorted\n",
    "\n",
    "    # Find the maximum of the difference curve. That point is the elbow/knee\n",
    "    max_difference_idx = np.argmax(dist_difference)\n",
    "\n",
    "    # Plot the distances curve and the difference curve\n",
    "    plt.plot(dist_sorted, label=\"K-NN Distance\")\n",
    "    plt.plot(dist_difference, label=\"Difference\")\n",
    "    plt.scatter(max_difference_idx, dist_sorted[max_difference_idx])\n",
    "    plt.scatter(max_difference_idx, dist_difference[max_difference_idx])\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Elbow is the max difference\n",
    "    return dist_sorted[max_difference_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elbow_dist = kneedle(MIN_SAMPLES_FIRST, dim_reduced_embeddings, plot=True)\n",
    "clusterer = DBSCAN(\n",
    "    eps=elbow_dist,\n",
    "    min_samples=MIN_SAMPLES_FIRST,\n",
    "    metric=\"cosine\"\n",
    ")\n",
    "\n",
    "clustering = clusterer.fit(\n",
    "    dim_reduced_embeddings,\n",
    "    sample_weight=final_comments[\"Likes\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame that includes the clustering information\n",
    "clustered_comments = final_comments.copy()\n",
    "clustered_comments[\"First Cluster\"] = clustering.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results of First Clustering\n",
    "\n",
    "The first pass at clustering allows us to find several meaningful clusters, and will let us remove ~8k of the ~33k points when we do a second pass. Hopefully those ~8k were acting as \"noise\" and when we clustering just the remaining samples it results in more meaningful topics\n",
    "\n",
    "Unforunately, most of the comments get grouped into clusters 0 and 1 (cluster 8 seems to be in a similar situation but is seperate and much smaller). The other clusters are well segmented and have low intra-cluster variance, which can easily be confirmed by checking samples from clusters 2 - 7.\n",
    "\n",
    "For clusters 2-7, we don't need TF-IDF to do topic modeling for each cluster because the comments within a cluster are incredibly simple and nearly identical.\n",
    "\n",
    "Cluster 0 & 1: Huge and diverse\n",
    "* Based on a reading a sample of comments in each of these clusters, each of these have many topics inside them\n",
    "    * This is evident by the top comments, but was confirmed by skimming more comments\n",
    "\n",
    "Clusters 2, 3 & 6: Mythpat Bhai\n",
    "* ~42k likes\n",
    "* These 3 clusters all relate to \"Mythpath Bhai\". They often have give the same phrase then have exclaimation points and/or emojis\n",
    "    * Cluster 2 is \"Mythpat Bhai\"\n",
    "    * Cluster 3 is \"Mythpath Bhai x Mr Beast\"\n",
    "    * Cluster 6 is \"Mission Mythpat Bhai\"\n",
    "* From what I can gather, Mythpath Bhai is name of another YouTuber and he voiced the Hindi dub of the video\n",
    "* I would speak to our stakeholders to confirm this and mention that comments referencing him have a total of ~33k Likes\n",
    "\n",
    "Cluster 4: Comments generally refer say \"Give him another chance\"\n",
    "* ~52k Likes\n",
    "\n",
    "Cluster 5: Fede Vigevani\n",
    "* ~7k Likes\n",
    "* This cluser refers to Fede Vigevani's dub of the video. Similar to the Mythpat Bhai\n",
    "* Also includes more generic references to other dubbings\n",
    "* Also includes several mentions of Klingon - Fede's dubbing of the video is labeled \"Klingon\"\n",
    "    * Some quick research showed this is not the first time it's been labeled Klingon, so that would be something to bring up as well\n",
    "\n",
    "Cluster 7: Same text repeated 9 times (with a collective ~3.6k likes). Text:<br />\n",
    "&emsp;M - Mighty in perseverance<br />\n",
    "&emsp;A - Always fearless<br />\n",
    "&emsp;C - Courageous and relentless<br />\n",
    "&emsp;K - Keep pushing forward<br />\n",
    "* ~4k likes\n",
    "\n",
    "Cluster 8: Generally about Mack, hard to immediately see any overarching topic\n",
    "\n",
    "Noise: Most comments are related to Khatron Ke Khiladi which seems to be an Indian version of Fear Factor. There are also several more \"Mythpat\" comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_labels = [idx for idx in clustered_comments[\"First Cluster\"].unique() if idx != -1]\n",
    "\n",
    "NUM_TOP_LIKED_SHOW = 5\n",
    "for cluster_label in cluster_labels:\n",
    "    # Get indices of points in the current cluster\n",
    "    cluster_idxs = np.where(clustered_comments[\"First Cluster\"] == cluster_label)[0]\n",
    "    \n",
    "    # Get the subset of the DataFrame for the cluster\n",
    "    cluster_df = clustered_comments.iloc[cluster_idxs]\n",
    "    \n",
    "    # Select the most liked comments in the cluster\n",
    "    top_liked = cluster_df.nlargest(NUM_TOP_LIKED_SHOW, \"Likes\")\n",
    "\n",
    "    intra_cluster_variance = calculate_intra_cluster_variance(dim_reduced_embeddings[cluster_idxs])\n",
    "\n",
    "    print(f\"Cluster {cluster_label}: {len(cluster_idxs)} comments | {sum(cluster_df[\"Likes\"])} likes | {np.round(intra_cluster_variance, 2):.2f} intra-cluster variance\")\n",
    "\n",
    "    # Print the most liked comments\n",
    "    for _, row in top_liked.iterrows():\n",
    "        print(f\"- ({row['Likes']} likes) {row['English Text']}\".replace(\"\\r\", \"\").replace(\"\\n\", \" \"))\n",
    "    print()\n",
    "\n",
    "# Print the number of noise points\n",
    "noise_count = (clustered_comments[\"First Cluster\"] == -1).sum()\n",
    "print(f\"Number of Noise Points: {noise_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Clustering\n",
    "\n",
    "For the next pass of clustering, I ignore samples that are well clustered based on the first pass of clustering. I.e. the samples in clusters 2 through 7.\n",
    "\n",
    "I perform a similar dimensionality reduction, but only using only the samples of interest so the mappings are better tuned to the remaining samples.\n",
    "\n",
    "I found that DBSCAN did not perform well on this subset of the data. While tuning the parameters it tended to do one of the following: label everything noise, label a substantial proportion noise and create 1-2 large clusters, or label nothing noise and create 1-2 large clusters. I performed hyperparameter tuning on UMAP's n_components and n_neighbors as well as DBSCAN's eps and min_samples. There were a small subset of hyperparameters that would have ~6 clusters. But the clusters didn't seem to be particularly meaningful. This may be because the first round of clustering got all the high density clusters, and DBSCAN looks for high density volumes to seed clusters.\n",
    "\n",
    "After DBSCAN didn't work, I turned to K-means clustering. Sklearn's implementation of K-means doesn't support weighted samples, so I just repeated samples based on the number of likes. I found that the raw number of likes was not great for this. Using the number of likes to be the sample weighted didn't work well. The top 10 comments have almost a 3rd of the total likes, so if we use the number of likes as the sample weighting, then the top N comments essentially become the centers for the clusters. The clusters then lose meaning because they are severely overfit on the most liked samples. To adjust this, I created a sublinear weighting function. The idea behind it is that we still want the number of likes to guide the clusters, but not directly proportionally. The sublinear function is linear from 1 to 10 (thanks to integer rounding), but then diverges more and more from a linear function. A comment with 100 likes has a weight of 62. The most liked comment has 114k likes, but only has a weight of 607.\n",
    "\n",
    "After trying some values for clusters between 10 and 100, I found that 40 gave interpretable results. If anything, 40 is probably too many clusters. However, it felt relatively easy to identify trends in the clusters by looking at the top comments and top TF-IDF topics (discussed below). By skimming many comments from each cluster, I was able to confirm that the top comments and TF-IDF topics were representative of the entire cluster for the most part. The purpose of clustering was to find trends in the comments without having to read tens of thousands of comments. While 40 clusters is still quite a bit of work to go through, it is at least manually tractible. If we needed many more clusters (say 400 instead of 40), I would have implemented other tricks to combine clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_cluster_idxs = np.where(np.isin(clustered_comments[\"First Cluster\"], [0, 1, 8]))[0]\n",
    "embeddings_second_cluster = embeddings[second_cluster_idxs]\n",
    "comments_second_cluster = clustered_comments.iloc[second_cluster_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensionality Reduction\n",
    "\n",
    "# Compute cumulative explained variance from PCA\n",
    "pca = PCA()\n",
    "pca.fit(embeddings_second_cluster)\n",
    "explained_variance_second = np.cumsum(pca.explained_variance_ratio_)\n",
    "explained_variance_second = np.insert(explained_variance_second, 0, 0.0)\n",
    "\n",
    "# 80% of explained variance\n",
    "EXPLAINED_VARIANCE_FIRST = 0.8\n",
    "n_components_second = np.argmax(explained_variance_second >= EXPLAINED_VARIANCE_FIRST)\n",
    "\n",
    "umap_model = UMAP(\n",
    "    n_components=n_components_second,\n",
    "    min_dist=0.0,\n",
    "    n_neighbors=5,\n",
    "    metric='cosine',\n",
    "    random_state=42,\n",
    "    n_jobs=1, # UMAP does not allow parallelism when seeded\n",
    "    low_memory=True\n",
    ")\n",
    "dim_reduced_embeddings_second_cluster = umap_model.fit_transform(embeddings_second_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sublinear_like_weighting(likes):\n",
    "    \"\"\" A sublinear function to modify the weight of likes \"\"\"\n",
    "    # From 1 to 10, the weight is linear (thanks to rounding)\n",
    "    # From 10 to 100, the weight begins to be more nonlinear (f(100) = 62)\n",
    "    # After 100, the function is highly sublinear. (f(100k) = 586)\n",
    "    denom = np.power(likes+100, 0.75)\n",
    "    return np.rint(33 * likes / denom).astype(int)\n",
    "\n",
    "# For K-means clustering, we need to repeat the samples based on the number of Likes\n",
    "#   We are only using the samples in clusters 0, 1 and 8 from the first round of clustering: embeddings_second_cluster\n",
    "#       In practice, it is the dimensionality reduced version: dim_reduced_embeddings_second_cluster\n",
    "# Create a copy of the embeddings with repeated samples: repeated_embeddings\n",
    "#       In practice, it is the dimensionality reduced version: dim_reduced_repeated_embeddings\n",
    "# There are N samples in first_embeddings_subset, and repeated_embeddings will have N unique values\n",
    "#   To make it easy to map back to the original dataframe, repeated_embeddings[:N] = embeddings_second_cluster\n",
    "indices = np.arange(len(comments_second_cluster))\n",
    "sample_weights = sublinear_like_weighting(comments_second_cluster[\"Likes\"])\n",
    "repeated_indices = np.hstack([np.full(count - 1, i) for i, count in enumerate(sample_weights) if count > 1])\n",
    "weighted_indices = np.concatenate([indices, repeated_indices])\n",
    "dim_reduced_repeated_embeddings = dim_reduced_embeddings_second_cluster[weighted_indices]\n",
    "\n",
    "# Perform clustering and get the labels\n",
    "NUM_K_MEANS_CLUSTERS = 40 # Hyperparameter\n",
    "kmeans = KMeans(n_clusters=NUM_K_MEANS_CLUSTERS, random_state=42, n_init=100)\n",
    "kmeans_clustering = kmeans.fit(dim_reduced_repeated_embeddings)\n",
    "clustered_comments.loc[second_cluster_idxs, \"Second Cluster\"] = kmeans_clustering.labels_[:len(second_cluster_idxs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modeling\n",
    "\n",
    "The topics of clusters were not always as immediately clear. We can use TF-IDF to try to find topics (which will guide our manual review of samples).\n",
    "\n",
    "TF-IDF has two terms Term Frequency (TF) and Inverse Document Frequency (IDF).\n",
    "* TF measures how often a topic appears in a comment - a topic is just an ngram in this case. I use ngrams of length 1, 2 and 3. It was uncommon for topics with 4+ words to score highly. Since the comments are so short, I use binary TF which just means 0/1 for does this topic appear in this comment at least once.\n",
    "* IDF is a normalization measure. Certain topics like \"he was\" are going to appear in many many comments. The IDF normalizes those topics to make the TF-IDF an importance measure.\n",
    "* As an example, if you run the TF-IDF on Shakespeare's 37 plays:\n",
    "    * You'll find the word \"Romeo\" only appears in one play, but it appears 130 times. So the word \"Romeo\" is both only present in a small number of documents (only 1 play), but it comes up a lot in that play. So its TF-IDF score is very high, and we would call that a topic.\n",
    "    * The word \"Falstaff\" is in 4 plays, and in one play appears 46 times. This would have a lower, but still quite high score. It might be a less prevalent topic.\n",
    "    * The word \"fool\" appears in 36 of the 37 plays, and in a large proportion of the plays it appears less than 10 times. Since the word \"fool\" appears in almost every play, it is just a word that Shakespeare likes. It is not a topic of any of those plays.\n",
    "\n",
    "To get the topics of each cluster, we can perform the IDF step on all comments, then perform the TF step on just the current cluster. Then we can sum the TFs across the comments of the current cluster and get the TF-IDF score of this cluster normalized across all comments.\n",
    "\n",
    "TF-IDF can have a problem with noise. To combat noise, we only consider a topic if it appears in more than 20 comments, but less than 10% of all comments (something like \"he is\" might appear in many comments and won't be represenative). We also don't want to consider stop words like \"the\", \"a\", \"on\", etc. There are some other tricks like lowercasing that are used as well. I also "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "After reading the top comments and topics for each cluster, I comfirmed by skimming many comments from that cluster that the top comments and topic were representative of the cluster. I gave what I though was the most salient topic of the cluster, and grouped simlar clusters together (there were obvious groupings). Part of what made this difficult to do automatically is that there was a lot of noise, and the comments were very short. If we had many clusters or different data, I could be smarter about automatically combining clusters.\n",
    "\n",
    "I am defining \"noise\" to be any comment that is generic enough that it could have been on most Mr Beast videos and still make sense. Examples:\n",
    "* 20:40 Insane\n",
    "* First comment!!!\n",
    "* Who's watching from Africa?\n",
    "* So sad! Can't believe it\n",
    "* At least give him $100k\n",
    "* < Quote from the video >\n",
    "\n",
    "#### Main Topics and Associated Clusters\n",
    "Variant of \"Another Chance\" (X Likes)\n",
    "* Cluster  0 - \"Mack deserved another try\"\n",
    "* Cluster  2 - \"This guy deserved another chance\"\n",
    "* Cluster  3 - \"Give him another chance Mr Beast\"\n",
    "* Cluster  4 - \"Jimmy, give Mack another chance\"\n",
    "* Cluster  5 - \"Give him another chance\" in a sentence with the word \"challenge\"\n",
    "* Cluster 12 - \"PLEASE give mack one more try. Hes so determined its inspiring\"\n",
    "* Cluster 13 - \"Give Mac another chance\"\n",
    "* Cluster 15 - \"Give him another chance, the underwater challenge was crazy\"\n",
    "* Cluster 16 - \"One more chance\"\n",
    "* Cluster 17 - \"Bring Mack back\"\n",
    "* Cluster 19 - \"Give him another chance\"\n",
    "* Cluster 21 - \"Bring him back\"\n",
    "* Cluster 29 - \"Give Mack another chance\"\n",
    "* Cluster 31 - \"Give this man another chance\"\n",
    "\n",
    "All about Mack (X Likes)\n",
    "* Cluster  6 - \"Mack should join the Mr Beast crew\"\n",
    "* Cluster 10 - \"Mack is the best contestant\"\n",
    "* Cluster 22 - \"Justice for Mack\"\n",
    "* Cluster 23 - \"Mack deserves the money\"\n",
    "* Cluster 25 - \"Mack is about to become a villian\"\n",
    "\n",
    "Challenges (X Likes)\n",
    "(The video was composed of several challenges that Mack had to complete to win, the last of which involved cookies)\n",
    "* Cluster 14 - Cookie Challenge\n",
    "* Cluster 38 - Challenges: \"Squid Game\" and \"Faced Fear\"\n",
    "* Cluster 39 - Cookie Challenge\n",
    "* Note: Cluster 15 could also go here\n",
    "\n",
    "Mr Beast's Advertisements ~116k Likes\n",
    "* Cluster 27 - Feastables/Chocolate\n",
    "\n",
    "Other People (X Likes)\n",
    "* Cluster  8 - Chandler's Reaction\n",
    "* Cluster 28 - Faith's Voice (referring to a person, not sure who this is) # TODO: rewatch and see if this is a member of Mr Beast's crew\n",
    "\n",
    "Noise (X Likes)\n",
    "* Cluster  1 - Timestamps\n",
    "* Cluster  7 - Country Related (\"Greetings from Peru\")\n",
    "* Cluster  9 - Seemingly unrelated comments\n",
    "* Cluster 11 - All about money (the prize was $800k)\n",
    "* Cluster 18 - \"Fear\"\n",
    "* Cluster 20 - Seemingly unrelated comments\n",
    "* Cluster 24 - 15 comments of \"Matt stop breaking my set\" (noise because it is just a quote from the video)\n",
    "* Cluster 26 - 8 total comments of \"Bro's dad lore is gonna be insane\"\n",
    "* Cluster 30 - Emojis and/or Yesssss/Noooooo\n",
    "* Cluster 32 - Seemingly unrelated comments\n",
    "* Cluster 33 - 1 comment: \"Who is Hindu?\"\n",
    "* Cluster 34 - Palestine\n",
    "* Cluster 35 - So sad\n",
    "* Cluster 36 - \"Anyone in _\" (Blank is November, December, etc)\n",
    "* Cluster 37 - Seemingly unrelated comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_df=0.10,\n",
    "    min_df=20,\n",
    "    ngram_range=(1, 3),\n",
    "    binary=True,\n",
    "    stop_words='english',\n",
    "    strip_accents=\"unicode\",\n",
    "    lowercase=True,\n",
    "\n",
    ")\n",
    "\n",
    "# Fit the idf step on the entire document corpus\n",
    "vectorizer.fit(clustered_comments[\"English Text\"].tolist())\n",
    "feature_names = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_labels = [idx for idx in clustered_comments[\"Second Cluster\"].unique() if idx != -1 and not np.isnan(idx)]\n",
    "cluster_labels.sort()\n",
    "\n",
    "cluster_centers = kmeans.cluster_centers_\n",
    "comments_second_clustering = clustered_comments.iloc[second_cluster_idxs]\n",
    "\n",
    "NUM_TOP_LIKED_SHOW = 5\n",
    "NUM_CENTROID_SHOW = 5\n",
    "for cluster_label in np.sort(comments_second_clustering[\"Second Cluster\"].unique()):\n",
    "    # Get indices of points in the current cluster\n",
    "    cluster_mask = comments_second_clustering[\"Second Cluster\"] == cluster_label\n",
    "    cluster_idxs = np.where(cluster_mask)[0]\n",
    "    cluster_embeddings = dim_reduced_embeddings_second_cluster[cluster_idxs]\n",
    "\n",
    "    # Get the subset of the DataFrame for the cluster\n",
    "    cluster_df = comments_second_clustering.iloc[cluster_idxs]\n",
    "\n",
    "    intra_cluster_variance = calculate_intra_cluster_variance(cluster_embeddings)\n",
    "    print(f\"Cluster {int(cluster_label)}: {len(cluster_idxs)} comments | {sum(cluster_df['Likes'])} likes | {np.round(intra_cluster_variance, 2):.2f} intra-cluster variance\")\n",
    "\n",
    "    # Print the most liked comments\n",
    "    print(\"Top Comments\")\n",
    "    top_liked = cluster_df.nlargest(NUM_TOP_LIKED_SHOW, \"Likes\")\n",
    "    for _, row in top_liked.iterrows():\n",
    "        print(f\"\\t- ({row['Likes']} likes) {row['English Text']}\".replace(\"\\r\", \"\").replace(\"\\n\", \" \"))\n",
    "\n",
    "    # Get the comments closest to the cluster centroid\n",
    "    print(\"Centroid Comments\")\n",
    "    cluster_center = cluster_centers[int(cluster_label)]\n",
    "    cluster_distances = np.sum(cluster_embeddings * cluster_center, axis=1)  # Efficient cosine similarity\n",
    "    closest_idx = cluster_idxs[np.argpartition(cluster_distances, -min(len(cluster_distances), NUM_CENTROID_SHOW))[-NUM_CENTROID_SHOW:]]\n",
    "    centroid_comments = comments_second_clustering.iloc[closest_idx]\n",
    "    for _, row in centroid_comments.iterrows():\n",
    "        print(f\"\\t- {row['English Text']}\".replace(\"\\r\", \"\").replace(\"\\n\", \" \"))\n",
    "\n",
    "    # Compute TF-IDF for this cluster\n",
    "    print(\"Top Topics\")\n",
    "    cluster_tfidf = vectorizer.transform(cluster_df[\"English Text\"].tolist())\n",
    "    cluster_tfidf = np.asarray(cluster_tfidf.sum(axis=0)).flatten()\n",
    "    cluster_tfidf_df = pd.DataFrame({\"Score\": cluster_tfidf, \"Topic\": feature_names})\n",
    "    cluster_tfidf_df = cluster_tfidf_df[cluster_tfidf_df[\"Score\"] > 0]\n",
    "    cluster_tfidf_df = cluster_tfidf_df.sort_values(\"Score\", ascending=False)\n",
    "    cluster_tfidf_df_subset = cluster_tfidf_df[cluster_tfidf_df[\"Topic\"].str.split().str.len() > 1]\n",
    "    max_width = cluster_tfidf_df[\"Topic\"].head(5).str.len().max()\n",
    "    if len(cluster_tfidf_df_subset[\"Topic\"]):\n",
    "        print(\"\\n\".join(f\"\\t{i+1}. {topic}\" for i, topic in enumerate(cluster_tfidf_df_subset[\"Topic\"].str.ljust(max_width).head(5))))\n",
    "    else:\n",
    "        print(\"\\tNo Topics\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Breakdown of engagement based on clusters\n",
    "\n",
    "We use likes as an approximation of viewer engagement. More likes means that topic is more engaged with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column with the categorizations\n",
    "categorizations = {\n",
    "    \"Another Chance\": np.isin(clustered_comments[\"Second Cluster\"], [0, 2, 3, 4, 5, 12, 13, 15, 16, 17, 19, 21, 29, 31]) | np.isin(clustered_comments[\"First Cluster\"], [4]),\n",
    "    \"All About Mack\": np.isin(clustered_comments[\"Second Cluster\"], [6, 10, 22, 23, 25]) | np.isin(clustered_comments[\"First Cluster\"], [7]),\n",
    "    \"Challenges\": np.isin(clustered_comments[\"Second Cluster\"], [14, 38, 39]),\n",
    "    \"Feastables\": np.isin(clustered_comments[\"Second Cluster\"], [27]),\n",
    "    \"Other People\": np.isin(clustered_comments[\"Second Cluster\"], [8, 28]),\n",
    "    \"Noise\": np.isin(clustered_comments[\"Second Cluster\"], [1, 7, 9, 11, 18, 20, 24, 26, 30, 32, 33, 34, 35, 36, 37]) | np.isin(clustered_comments[\"First Cluster\"], [-1]),\n",
    "    \"Mythpat Bhai\": np.isin(clustered_comments[\"First Cluster\"], [2, 3, 6]),\n",
    "    \"Fede Vigevani\": np.isin(clustered_comments[\"First Cluster\"], [5])\n",
    "}\n",
    "clustered_comments[\"Final Topic\"] = np.select(list(categorizations.values()), list(categorizations.keys()), default=\"Uncategorized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "clustered_comments[\"Final Topic\"].value_counts().plot.pie(\n",
    "    startangle=0.0,\n",
    "    labeldistance=1.05,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: query embedding on ads\n",
    "Feastables\n",
    "* chocolate\n",
    "* Featables\n",
    "* walmart\n",
    "\n",
    "Shopify\n",
    "* shopify\n",
    "* shopping\n",
    "* online shopping"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
