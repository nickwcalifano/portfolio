{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mission Statement\n",
    "* In this simulation, a popular YouTuber has asked us to find the top 10 things talking about in the comments on one of his videos\n",
    "* He wants to use this information to see what caught people's attention, if people are talking about the adverts in the video, and anything else useful we might find\n",
    "\n",
    "General Directions from the YouTuber:\n",
    "* We want the number of likes on a comment to inform the process somehow\n",
    "* There are obviously more than 10 things dicussed in the comments, but he wants the top 10\n",
    "    * If there's a good reason for more, he wants more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Process\n",
    "* Pull the data from Youtube\n",
    "    * Get all comments and the number of likes for each comment\n",
    "* Clean the data\n",
    "    * Remove any duplicates\n",
    "        * The pinned comment is the only a true duplicate\n",
    "        * If the same comment is posted by different users, combine it into one comment with the total number of likes\n",
    "    * Translate all comments to English\n",
    "* Get text embedding of each unique comment\n",
    "* Perform dimensionality reduction\n",
    "    * Clustering algorithms generally perform better in relatively low dimensional space\n",
    "* Cluster the comments\n",
    "    * Use a weighted clustering clustering algorithm with the number of likes acting as the sample weight\n",
    "* Perform topicl modeling (TF-IDF) to determine the topic of each cluster\n",
    "* Depending on the number of clusters/topics, we can report the \"top 10 things\"\n",
    "    * If there are more than 10 \"things\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from pyyoutube import Api\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.translation.text import TextTranslationClient\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "# APIs\n",
    "GOOGLE_API_KEY = os.environ.get(\"GOOGLE_KEY\")\n",
    "AZURE_KEY = os.environ.get(\"AZURE_KEY\")\n",
    "AZURE_ENDPOINT = \"https://api.cognitive.microsofttranslator.com\"\n",
    "AZURE_REGION = \"southcentralus\"\n",
    "\n",
    "# Video ID\n",
    "VIDEO_ID = \"KOEfDvr4DcQ\"\n",
    "\n",
    "# Save the intermediate steps so we don't have to do reprocessing\n",
    "CACHE_DIR = \"./data/youtube_comments\"\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "RAW_COMMENTS_PATH = os.path.join(CACHE_DIR, VIDEO_ID + \"_raw.csv\")\n",
    "CLEANED_COMMENTS_PATH = os.path.join(CACHE_DIR, VIDEO_ID + \"_cleaned.csv\")\n",
    "TRANSLATED_COMMENTS_PATH = os.path.join(CACHE_DIR, VIDEO_ID + \"_translated.csv\")\n",
    "FINAL_COMMENTS_PATH = os.path.join(CACHE_DIR, VIDEO_ID + \"_final.csv\")\n",
    "EMBEDDINGS_PATH = os.path.join(CACHE_DIR, VIDEO_ID + \"_embeddings.npy\")\n",
    "\n",
    "# Model IDs\n",
    "EMBEDDING_MODEL_ID = \"Alibaba-NLP/gte-Qwen2-7B-instruct\"\n",
    "EMBEDDING_MAX_LENGTH = 8192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect the data\n",
    "# The text to be clustered are all comments on Mr Beast's most watched video of 2024\n",
    "#   His most watched video of 2024 as of 02/2025 is \"Face Your Biggest Fear To Win $800,000\"\n",
    "# At time of data collection, the video has ~300M views and ~215k comments\n",
    "\n",
    "# If the comments have already been pulled and saved, just load them\n",
    "if os.path.exists(RAW_COMMENTS_PATH):\n",
    "    raw_comments = pd.read_csv(RAW_COMMENTS_PATH)\n",
    "    raw_comments[\"Text\"] = raw_comments[\"Text\"].fillna(\"\")\n",
    "\n",
    "# If they haven't been saved, pull them then save them\n",
    "else:\n",
    "    responses = []\n",
    "    token = None\n",
    "\n",
    "    # Get a batch of 1000 comment threads\n",
    "    # response is a linked list. If response.nextPageToken is not None, there are more comments\n",
    "    # Get responses until there are no more (response.nextPageToken is None)\n",
    "    api = Api(api_key=GOOGLE_API_KEY)\n",
    "    while True:\n",
    "        response = api.get_comment_threads(\n",
    "            video_id=VIDEO_ID,\n",
    "            count=1000,\n",
    "            page_token=token,\n",
    "            text_format=\"plainText\"\n",
    "        )\n",
    "\n",
    "        # Append and get the next page\n",
    "        responses.append(response)\n",
    "        token = response.nextPageToken\n",
    "\n",
    "        # If this is the last page, break\n",
    "        if not token:\n",
    "            break\n",
    "    \n",
    "    # Get the text, number of likes, and ID of each comment. Save it to a csv\n",
    "    raw_comments = [\n",
    "        (thread.snippet.topLevelComment.snippet.textDisplay, thread.snippet.topLevelComment.snippet.likeCount, thread.id)\n",
    "        for response in responses for thread in response.items\n",
    "    ]\n",
    "    raw_comments = pd.DataFrame(raw_comments, columns=[\"Text\", \"Likes\", \"Id\"])\n",
    "    raw_comments.to_csv(RAW_COMMENTS_PATH, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "* Of the ~215k comments, there are only ~144k unique comment texts (many repeated comments)\n",
    "    * This seems to be mostly a combination of bots reposting comments, and trivial messages (many times just an emoji)\n",
    "    * However, it is also the same (non-trival) comment being naturally posted several times\n",
    "* We can combine two rows if their text is the same, and add their likes together\n",
    "* Of the 144k unique comments, only ~33k have at least 1 like\n",
    "    * We can assume the unliked comments are noise, so we can remove them\n",
    "* Since we will be doing topic modeling with TF-IDF we want all comments to be in English\n",
    "    * To translate, we can just run all the comments through Microsoft Cognitive Services Text Translation\n",
    "    * Only ~70% of the comments were English\n",
    "* After translating, there are duplicates. We can combine them just as before\n",
    "    * \"Hola\" and \"Hello\" wouldn't originally be combined, but after translating both now say \"Hello\" and can be combined\n",
    "    * We can combine an additional ~1.8% of the ~33k comments after translating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicated and comments with 0 likes\n",
    "if os.path.exists(CLEANED_COMMENTS_PATH):\n",
    "    cleaned_comments = pd.read_csv(CLEANED_COMMENTS_PATH)\n",
    "    cleaned_comments[\"Text\"] = cleaned_comments[\"Text\"].fillna(\"\")\n",
    "\n",
    "else:\n",
    "    # Mr Beast's pinned comment is the only true duplicated comment\n",
    "    cleaned_comments = raw_comments.drop_duplicates()\n",
    "\n",
    "    # Combine two or more comments if their text is identical. Sum the likes from all combined comments\n",
    "    #   Keep the ID of the comment with more likes\n",
    "    cleaned_comments = cleaned_comments.sort_values(by=\"Likes\", ascending=False)\n",
    "    cleaned_comments = cleaned_comments.groupby(\"Text\", as_index=False).agg({\n",
    "        \"Likes\": \"sum\",\n",
    "        \"Id\": \"first\"\n",
    "    })\n",
    "\n",
    "    # Most comments still have 0 likes. Anything with zero noise is considered noise\n",
    "    cleaned_comments = cleaned_comments[cleaned_comments[\"Likes\"] > 0]\n",
    "\n",
    "    # Save the cleaned comments to disk\n",
    "    cleaned_comments.to_csv(CLEANED_COMMENTS_PATH, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate all comments to English\n",
    "#   We need all comments in English so we can properly perform TF-IDF later\n",
    "#   We want to translate before embedding since language is likely represented in the embedding\n",
    "#       Unfortunately, the translation isn't perfect, but it will be good enough\n",
    "# We translate after combining so we don't translate the same exact piece of text multiple times\n",
    "\n",
    "# Free tier of translation service has a rate limit of 33k characters per minute. The documentation only mentions a 2M\n",
    "#   limit per hour, but if I send more than 33k characters per minute (2M per hour average) I get an \"exceeded request\n",
    "#   limits\" error.\n",
    "# To make the code as simple as possible since it only needs to run once, I found that batching the requests into 200\n",
    "#   comments never exceeds 33k characters. I can then wait a minute between each batch to avoid the rate limit.\n",
    "# If this code was going to be run repeatedly, I'd use a variable batch size and pack each batch based on the character\n",
    "#   count of each comment in the batch.\n",
    "def language_and_enlish_translation(texts, batch_size = 200):\n",
    "    \"\"\" Given a list of texts returns their source language and the translation of each item to English \"\"\"\n",
    "    client = TextTranslationClient(credential=AzureKeyCredential(AZURE_KEY), region=AZURE_REGION, endpoint=AZURE_ENDPOINT)\n",
    "    response = []\n",
    "\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        texts_batch = texts[i:i + batch_size]\n",
    "        response += client.translate(body=texts_batch, to_language=[\"en\"])\n",
    "        time.sleep(60)\n",
    "\n",
    "    return [\n",
    "        (item[\"detectedLanguage\"][\"language\"], item[\"translations\"][0][\"text\"])\n",
    "        for item in response\n",
    "    ]\n",
    "\n",
    "if os.path.exists(TRANSLATED_COMMENTS_PATH):\n",
    "    translated_comments = pd.read_csv(TRANSLATED_COMMENTS_PATH)\n",
    "    translated_comments[[\"Text\", \"English Text\"]] = translated_comments[[\"Text\", \"English Text\"]].fillna(\"\")\n",
    "\n",
    "else:\n",
    "    translated_comments = cleaned_comments.copy()\n",
    "    translated_comments.loc[:, [\"Language\", \"English Text\"]] = language_and_enlish_translation(translated_comments[\"Text\"].tolist())\n",
    "    translated_comments.to_csv(TRANSLATED_COMMENTS_PATH, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pie Chart of Distribution of Languages of Unique Comments with 1 or more Likes\n",
    "\n",
    "# Top Languages to show\n",
    "language_map = {\n",
    "    \"en\": \"English\",\n",
    "    \"es\": \"Spanish\",\n",
    "    \"ru\": \"Russian\",\n",
    "    \"so\": \"Somali (Arabic)\",\n",
    "    \"ar\": \"Arabic\",\n",
    "    \"pt\": \"Portuguese (Brazil)\",\n",
    "}\n",
    "\n",
    "# Get the count per language. Combine less common languages into \"Other\"\n",
    "# Note:\n",
    "#   This is the count of unique comments with 1+ likes\n",
    "#   We already removed duplicate texts and comments wtih 0 likes\n",
    "language_counts = translated_comments['Language'].replace(language_map).value_counts()\n",
    "top_languages = language_counts.nlargest(len(language_map))\n",
    "other_count = language_counts.iloc[len(language_map):].sum()\n",
    "top_languages['Other'] = other_count\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "explode = [0] + [0.1]*len(language_map)\n",
    "top_languages.plot.pie(\n",
    "    ax=ax,\n",
    "    startangle=54.36,\n",
    "    explode=explode,\n",
    "    labeldistance=1.03,\n",
    "    autopct=\"%.1f%%\",\n",
    "    pctdistance=0.80\n",
    ")\n",
    "ax.set_ylabel('')\n",
    "fig.suptitle(\"Distribution of Languages\\nof Unique Comments with 1+ Likes\", x=0.55, y=0.95)\n",
    "ax.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pie Chart of Total Likes per Language\n",
    "language_map = {\n",
    "    \"en\": \"English\",\n",
    "    \"es\": \"Spanish\",\n",
    "}\n",
    "\n",
    "# Get the count of likes per language. Combine less common languages into \"Other\"\n",
    "# Note:\n",
    "#   We preserved likes when removing duplicate texts and only removed comments with 0 likes\n",
    "#   So the count of likes per language is the same here as it would be before any data filtering\n",
    "likes_per_language = translated_comments.groupby('Language')['Likes'].sum()\n",
    "likes_per_language = likes_per_language.sort_values(ascending=False)\n",
    "likes_per_language = likes_per_language.rename(index=language_map)\n",
    "top_likes = likes_per_language.nlargest(len(language_map))\n",
    "other_likes = likes_per_language.iloc[len(language_map):].sum()\n",
    "top_likes['Other'] = other_likes\n",
    "\n",
    "# Plot the pie chart\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "explode = [0] + [0.1]*len(language_map)\n",
    "top_likes.plot.pie(\n",
    "    ax=ax,\n",
    "    startangle=5.6,\n",
    "    labeldistance=1.03,\n",
    "    explode=explode,\n",
    "    autopct=\"%.1f%%\",\n",
    "    pctdistance=0.80\n",
    ")\n",
    "ax.set_ylabel('')\n",
    "fig.suptitle(\"Total Likes per Language\", x=0.525, y=0.90)\n",
    "ax.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine again after translating\n",
    "# Now that everything is English, there may be more repeated comments\n",
    "#   If the same comment was in two different languages, by translating all to English they would now be redundant\n",
    "#   Previously \"Hola\" and \"Hello\" would not be combined. After translating both would be \"Hello\" and would be combined\n",
    "\n",
    "if os.path.exists(FINAL_COMMENTS_PATH):\n",
    "    final_comments = pd.read_csv(FINAL_COMMENTS_PATH)\n",
    "    final_comments[[\"Text\", \"English Text\"]] = final_comments[[\"Text\", \"English Text\"]].fillna(\"\")\n",
    "\n",
    "else:\n",
    "    # Combine if the English Text is the same and sum the likes\n",
    "    final_comments = translated_comments.copy()\n",
    "    final_comments = final_comments.sort_values(by=\"Likes\", ascending=False)\n",
    "    final_comments = final_comments.groupby(\"English Text\", as_index=False).agg({\n",
    "        \"Likes\": \"sum\",\n",
    "        \"Text\": \"first\",\n",
    "        \"Language\": \"first\",\n",
    "        \"Id\": \"first\"\n",
    "    })\n",
    "\n",
    "    # Reset the index after cleaning is complete\n",
    "    final_comments = final_comments.reset_index(drop=True)\n",
    "    \n",
    "    # Save the final comments to disk\n",
    "    final_comments.to_csv(FINAL_COMMENTS_PATH, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print top 10 most liked comments\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "pd.set_option(\"display.width\", 1000)\n",
    "final_comments[[\"English Text\", \"Likes\"]].nlargest(10, \"Likes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of Tokenized Lengths\n",
    "tokenizer = AutoTokenizer.from_pretrained(EMBEDDING_MODEL_ID)\n",
    "token_lengths = [len(tokenizer.encode(comment)) for comment in final_comments[\"English Text\"]]\n",
    "plt.hist(token_lengths,\n",
    "    bins=np.arange(100) - 0.5,\n",
    "    density=True\n",
    ")\n",
    "plt.show()\n",
    "print(\"Percentage longer than 100:\", np.round(sum(1 for x in token_lengths if x > 100) / len(token_lengths) * 100, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of Likes\n",
    "* There are ~1.1M likes between all 33k comments.\n",
    "* Looking at the histogram, we see most of the comments have only a few likes\n",
    "    * The first 100 comments are omitted from the histogram to make it more readable\n",
    "    * The comment with the most likes is Mr Beast's pinned comment with ~114k likes\n",
    "        * There are only 10 comments with more than 10k likes. These 10 comments have ~330k likes total (31% of all comment likes)\n",
    "        * There are only 25 comments with more than 5k likes. These 25 comments have ~430k likes total (40% of all comment likes)\n",
    "        * There are only 100 comments with more than 1600 likes. These 100 comments have ~630k likes total (59% of all comment likes)\n",
    "* Lookin at the cumulative sum of likes vs numbner of top comments\n",
    "    * The top 1 comment has ~11% of all comment likes\n",
    "    * The top 1% of comments have 77% of all comment likes\n",
    "    * The top 2% of comments have 86% of all comment likes\n",
    "    * The top 5% of comments have 93% of all comment likes\n",
    "    * The top 10% of comments have 96% of all comment likes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of number of likes\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(final_comments[\"Likes\"], bins=np.arange(0, 1600, 16), log=True)\n",
    "plt.xlabel(\"Number of Likes\")\n",
    "plt.ylabel(\"Frequency (Log Scale)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cumulative Distribution of Likes Plot\n",
    "sorted_likes = final_comments[\"Likes\"].sort_values(ascending=False).to_numpy()\n",
    "cumulative_likes = np.cumsum(sorted_likes) / sum(sorted_likes)\n",
    "cumulative_likes = np.insert(cumulative_likes, 0, 0.0)\n",
    "\n",
    "plt.plot(np.linspace(0, 1, len(cumulative_likes), endpoint=True), cumulative_likes)\n",
    "plt.grid()\n",
    "plt.xlabel(\"Number of Top Comments Considered\")\n",
    "plt.ylabel(\"Cumulative Proportion of Likes\")\n",
    "plt.title(\"Cumulative Distribution of Likes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# If using a model with query/document, use default if it is document (also sometimes called passage). Probably always use document\n",
    "https://huggingface.co/spaces/mteb/leaderboard\n",
    "* Clustering task uses kmeans (with eucliudean distance) - so good performance there indicates good performance for this application\n",
    "\n",
    "* MTEB is a benchmark of many embedding tasks. \n",
    "* filtered down to just the Clustering task (several datasets)\n",
    "* Within clustering, the most related task is Stack Exchange as it is the only data collected from web domain (datasets from online sources such as wikipedia or similar are considered encyclopaedic, non-fiction, etc). The Stack Exchange dataset is unfortuntely entirely english. The youtube comments are primarily english, but so have other languages\n",
    "* Top 2 performing models on the Stack Overflow clustering task (as of the time of writing this) are gte_Qwen1.5-7B-instruct and gte-Qwen2-7B-instruct with scores of 80.60 and 80.26 respectively\n",
    "* The scores are likely close enough that the Stack Exchange Clustering along could not definitively tell us which model would be better on our dataset\n",
    "* Given that, I have chosen to use the Qwen2 model as it generally has better performance than the Qwen1.5 model\n",
    "\n",
    "MTEB just uses normal kmeans:\n",
    "clustering_model = sklearn.cluster.MiniBatchKMeans(\n",
    "    n_clusters=len(set(self.labels)),\n",
    "    batch_size=self.clustering_batch_size,\n",
    "    n_init=\"auto\",\n",
    ")\n",
    "clustering_model.fit(corpus_embeddings)\n",
    "cluster_assignment = clustering_model.labels_\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"./final_comments.pickle\", \"wb\") as f:\n",
    "    pickle.dump(final_comments, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"./final_comments.pickle\", \"rb\") as f:\n",
    "    final_comments = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(EMBEDDINGS_PATH):\n",
    "    embeddings = np.load(EMBEDDINGS_PATH)\n",
    "else:\n",
    "    model = SentenceTransformer(EMBEDDING_MODEL_ID, trust_remote_code=True)\n",
    "    model.max_seq_length = EMBEDDING_MAX_LENGTH\n",
    "    embeddings = model.encode(final_comments[\"English Text\"].tolist(), batch_size=1, show_progress_bar=True)\n",
    "    np.save(EMBEDDINGS_PATH, embeddings)\n",
    "    del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO:\n",
    "* PCA for variance explainability to show dimensionality vs information\n",
    "    * Show that we can reduce down to relatively small dimensionality (cut 99% of dimensions) without losing same proportion of information (less than 99% of information lost, should be likse 20% of information lost)\n",
    "\n",
    "* UMAP on original embeddings down to to 10 dimensional space\n",
    "    * Make subplot image like \"UMAP Parameters\" parameters section of https://pair-code.github.io/understanding-umap/\n",
    "        * min_dist from 0, 0.01, 0.05, 0.1, 0.5, 1\n",
    "        * n_neighbors from 5, 15, 30, 50, 100\n",
    "\n",
    "* DBSCAN\n",
    "    * Sklearn's implementation of DBSCAN comes out of the box with sample weighting\n",
    "    * DBSCAN will also \"find\" the number of clusters, so we don't have to know apriori. DBSCAN also works with noise and we expect most of the comments to be noise\n",
    "    * For eps, Two methods\n",
    "        * K-distance plot\n",
    "            * Plot the distance to the k-th nearest neighbor and look for an elbow in the graph\n",
    "            * Elbow plot\n",
    "            * \"Kneedle Algorithm\" says elbow is where the largest change is slope of the curve occurs (largest 2nd derivative)\n",
    "            * code:\n",
    "            from sklearn.neighbors import NearestNeighbors\n",
    "            k = 5\n",
    "            nbrs = NearestNeighbors(n_neighbors=k).fit(embeddings)\n",
    "            distances, indices = nbrs.kneighbors(embeddings)\n",
    "            distances = np.sort(distances[:, k-1], axis=0)\n",
    "            gradients = np.diff(distances)\n",
    "            sharpest_gradient_index = np.argmax(np.diff(gradients))\n",
    "        * silhouette_score (preferred?)\n",
    "            * grid search over eps (and maybe min_samples)\n",
    "    * For min_samples, maybe use some proportion of the Likes.\n",
    "        * I.e. we don't want to consider a cluster if it has less than 1% of all likes\n",
    "\n",
    "* Grid search over min_dist, n_neighbors, and eps (set min_samples to something like 5.5k which is 0.5% of all Likes)\n",
    "    * Plot most informative 2 variables (best eps for a given min_dist or n_neighbors, best min_dist for a given eps and n_neighbors, etc)\n",
    "        * for a given eps and n_neighbors, min_dist is likely to be optimal at low values like 0 or 0.01. So we will likely want to have the plot be eps vs n_neighbors and we can pick the best min_dist since we know we always want a low value\n",
    "\n",
    "* Topic Modeling (Naturally Occuring Topics)\n",
    "    * Weighted TF-IDF\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    vectorizer = TfidfVectorizer(max_df=0.99, min_df=2, stop_words='englihs', ngram_range=(1, 5))\n",
    "    # idf to normalize occurences of words across all samples\n",
    "    vectorizer.fit(final_comments[\"English Text\"].tolist()) # to do weighting?\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    X_cluster = vectorizer.transform(cluster[\"English Text\"].tolist()) # for each cluster (I think this is document frequency)\n",
    "    X_sum = np.asarray(X_cluster.sum(axis=0)).flatten() # for each cluster (I think this is cluster frequency)\n",
    "    X_df = pd.DataFrame({'ngram': ferature_names, 'score: X_sum}) # topic and tf-idf score\n",
    "\n",
    "* TODO: Topic Modeling\n",
    "    * Shoud stemming/lemmatization be performed?\n",
    "    *   Does mmr take care of this?\n",
    "\n",
    "* MMR - Maximal Marginal Relevance\n",
    "    * https://github.com/MaartenGr/BERTopic/blob/master/bertopic/representation/_mmr.py\n",
    "    * Fine tuning of topic modeling\n",
    "    * Take top topics (ngrams) out of TF-IDF\n",
    "    * Assume there are similar topics (Ex: car and cars)\n",
    "    * Find subset of topic models that maximize mmr\n",
    "    * I think we want to use the query embeddings for the words here\n",
    "\n",
    "* Topic Modeling (Artificial Topics)\n",
    "    * embedding model has a query embedding\n",
    "    * Get a gauge of the intra-cluster distance for each natural cluster\n",
    "    *   Probably mean intra-cluster distance and mean distance to cluster centroid\n",
    "    *       Maybe add 2-3 standard deviations to mean as well\n",
    "    * Get query embedding for samples near \"Feastables\", \"Feastables Chocolate\", \"Shopify, \"Advertisement\" etc\n",
    "    * For each query embedding:\n",
    "        * Find all comments that are within some distance from the query embedding\n",
    "            * Natural intra-cluster distance is a good start for \"some distance\"\n",
    "            * Note: a comment can be in more than one of these artifical clusters\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
