{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Automatic Summary Evaluation\n",
    "\n",
    "This project demonstrates an LLM-as-a-judge technique to evaluate summaries generated by machine learning models. Summary evaluation is useful application of LLMs in production environments and has analogies in similar evaluations of GenAI outputs (or the automated evaluation of human outputs).\n",
    "\n",
    "This notebook uses data from SummEval [1] which is a dataset of CNN and Daily Mail articles, machine generated summaries of those articles, and human evaluations of those summaries. In this notebook, a G-Eval technique is used to evaluate the summaries on consistency. G-Eval is a powerful tool with relatively high correlation with human evaluations with the benefit that it can be used with a foundational model and does not require fine-tuning to achieve good results. It is especially powerful when the metric otherwise difficult to score - How would you the define the \"relevance\" of a summary using a rules-based-nlp method? How do you get enough \"relevance\" examples to train an NLP model?\n",
    "\n",
    "G-Eval [2] was originally used to better measure Relevance, Consistency, Fluency, and Coherence. Relevance being the measure of how well the summary captures the key points of the article. Consistency being the measure of whether the summary reproduces all facts accurately and does not make up untrue information. Fluency being the measure of the quality of individual sentences (grammatically correct). Coherence being the measure of the quality of the summary as a whole.\n",
    "\n",
    "In general, machine written summaries are highly Fluent and Coherent [3] - to the point that focus as can shift entirely to Relevance and Consistency. Relevance is not as \"solved\" as Fluency or Coherence, but LLMs often produce output with acceptable levels of relevance. The biggest struggle for LLMs is often consistency - coming from an incorrect representation of events in the source text. These incorrect representations are known as hallucinations and can be an incorrect description of events that contradict the source (known as intrinsic hallucinations), or the introduction of new information that cannot be verified by the source (extrinsic hallucinations). that being said, SummEval was published in 2020 and the summaries of the dataset were produced with non-LLM models and are therefore not always fully fluent or coherent.\n",
    "\n",
    "This implementation of G-Eval uses several custom prompts as well as a modified version of an example prompt from Microsoft's Prompt Flow [4]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import Union, Tuple, Dict, Callable\n",
    "\n",
    "from numpy.typing import ArrayLike\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr, spearmanr, kendalltau\n",
    "\n",
    "import llm_utils\n",
    "from llm_utils import LlmUtils\n",
    "\n",
    "# Dataset paths\n",
    "SUMM_EVAL_DATA = \"./data/summEval/summeval.json\"\n",
    "\n",
    "# Cached request/responses\n",
    "CACHE_PATH = \"./data/summEval/llm_cache.pickle\"\n",
    "\n",
    "# Async Parameters\n",
    "NUM_PARALLEL_CONNS_LLAMA = 3\n",
    "NUM_PARALLEL_CONNS_GPT = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Dataset\n",
    "\n",
    "This dataset is from SummEval and is comprised of full text (source) articles from CNN and Daily Mail, summaries generated by various NLP models, and human evaluations of those summaries.\n",
    "\n",
    "For this notebook, the only score we are interested in is the consistency score. Consistency can have multiple definitions in the literature and in practice, but the definition given to the human evaluators was: \"The [Consistency] rating measures whether the facts in the summary are consistent with the facts in the original article. Consider whether the summary does reproduce all facts accurately and does not make up unture information.\n",
    "\n",
    "It has been several years since SummEval has been published. Since then, much of the literature has broken consistency into 2 metrics: faithfulness (also known as groundedness) and coverage (also known as completeness or recall). This notebook will focus on consistency since there is not ground truth data for faithfulness or coverage in the SummEval dataset.\n",
    "\n",
    "However, it is an interesting, almost philosophical, discussion to be had about faithfulness and coverage. Domain spcific language, especially with easy to understand definitions, can often be useful when discussing with stakeholders because it can avoid miscomunications.\n",
    "\n",
    "Faithfulness: Faithfulness is defined as staying consistent and truthful to the provided source, and we do not consider ‘factuality’ where valid external facts are acceptable [3]. A summary that omits details but isn't otherwise misleading or inaccuracte would still be faithful. Not considering factuality means a factually inaccurate piece of information in the source should be faithfully represented in the summary. Therefore a faithful summary of a flat-earther article would have the same factual inaccuracies that are present in the source article.\n",
    "\n",
    "Coverage: Coverage is defined as the comprehensiveness of the summary and quantifies how well a summary captures and accurately represents key information from the source. A summary that contains all information from the text, but hallucinates new information, so long as the hallucinations do not mislead or inaccurately represent the source, would still have high coverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data used for evaluation available from the G-Eval paper's GitHub repo\n",
    "#   https://raw.githubusercontent.com/nlpyang/geval/refs/heads/main/data/summeval.json\n",
    "summ_eval = json.load(open(SUMM_EVAL_DATA))\n",
    "summ_eval = pd.DataFrame(summ_eval)\n",
    "summ_eval[\"consistency\"] = summ_eval[\"scores\"].apply(lambda x: x[\"consistency\"])\n",
    "summ_eval.drop([\"doc_id\", \"system_id\", \"reference\", \"scores\"] , axis=1, inplace=True)\n",
    "summ_eval.rename(columns={\"source\": \"article\", \"system_output\": \"summary\"}, inplace=True)\n",
    "\n",
    "# 5 articles (and all 16 summaries associated with each of the articles) get flagged by the Azure OpenAI Service'set\n",
    "#   Content Filter, even on the most permissive settings\n",
    "# This filter can be removed for trusted users so long as they still uphold Microsoft's content moderation policies\n",
    "#   My personal account is not a trusted user, so I cannot remove the content filter\n",
    "# I enumerate each index explicitly because it would be possible for a specific summary to be flagged, but the other\n",
    "#   summaries associated with the same article. This makes it easier to apply to a new datraset in the future\n",
    "azure_error_indices = [\n",
    "      80,   81,   82,   83,   84,   85,   86,   87,   88,   89,   90,   91,   92,   93,   94,   95,\n",
    "     352,  353,  354,  355,  356,  357,  358,  359,  360,  361,  362,  363,  364,  365,  366,  367,\n",
    "     448,  449,  450,  451,  452,  453,  454,  455,  456,  457,  458,  459,  460,  461,  462,  463,\n",
    "    1472, 1473, 1474, 1475, 1476, 1477, 1478, 1479, 1480, 1481, 1482, 1483, 1484, 1485, 1486, 1487,\n",
    "    1520, 1521, 1522, 1523, 1524, 1525, 1526, 1527, 1528, 1529, 1530, 1531, 1532, 1533, 1534, 1535\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calling the models\n",
    "\n",
    "This notebook compares the performance of 4 models on 4 prompts.\n",
    "\n",
    "The models are Llama3.2-vision [5], Phi4 [6], DeepSeek-R1-Distill-Qwen-14B [7], and GPT-4o mini [8]. \n",
    "* Llama3.2-vision is an 11B parameter model released by Meta in 09/2024\n",
    "* Phi4 is a 14B parameter model released by Microsoft in 12/2024\n",
    "* DeepSeek-R1-Distill-Qwen-14B uses the Qwen-14B model architecture and was distilled from DeepSeek-R1 (released by DeepSeek in 01/2025)\n",
    "* GPT 4o mini is OpenAI's \"most cost-efficeint small model\" and was released in 07/2024.\n",
    "\n",
    "All models except GPT-4o mini are open source and are called using a local OLLAMA server. GPT-4o mini is called using the Azure OpenAI Service. It would be easy to compare more models, especially ones available on Ollama or Azure OpenAI Service. The open source models were selected because they are the most performative (and largest) models that I can reasonably run on my personal GPU. GPT-4o mini was selected because it is the most performative model for which I am willing to pay the monetary cost. GPT-4o and o1-mini are both more than an order of magnitude more expensive and o1 is nearly 2 orders of magnitude more expensive.\n",
    "\n",
    "Both OLLAMA and the Azure OpenAI Service use a similar REST API endpoints, but there are some small differences between the APIs. All differences are contained and managed by llm_utils."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the calls to the LLMs get saved to a 'cache', just a dictionary with requests as keys and responses as values\n",
    "# This is useful when rerunning and adding new models or otherwise experimenting or debugging since the calls to the\n",
    "#   LLMs are expensive both in time and for GPT, money. \n",
    "\n",
    "# I using a try-except in this manner so I can re-run the full notebook without accidentally overwritting the cache.\n",
    "# The cache is pickled and saved to disk once all samples are passed through a prompt-model pair\n",
    "try:\n",
    "    CACHE\n",
    "except:\n",
    "    if os.path.exists(CACHE_PATH):\n",
    "        with open(CACHE_PATH, \"rb\") as f:\n",
    "            CACHE = pickle.load(f)\n",
    "    else:\n",
    "        CACHE = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the request payload for the model calls\n",
    "# Uses structured outputs to ensure the model gives a output than can be parsed\n",
    "def get_request_payload(\n",
    "        model_name: llm_utils.AllowedModelNames,\n",
    "        prompt: Union[str, Tuple[str, str]]\n",
    ") -> dict:\n",
    "    \"\"\" Creates the request payload for the given model\n",
    "    \"\"\"\n",
    "    # Set some fields based on the inputs\n",
    "    model_type = LlmUtils.get_model_type(model_name)\n",
    "    num_output_tokens = 10\n",
    "    seed = 314159265\n",
    "\n",
    "    # If randomness options were not supplied, set them to minimum values\n",
    "    temperature = 0.0\n",
    "    top_p = 0.0\n",
    "    top_k = 1\n",
    "\n",
    "    # Create the portions of the request payload that are identical between Ollama and Azure OpenAI\n",
    "    payload = {\n",
    "        \"stream\": False,\n",
    "    }\n",
    "\n",
    "    # Create the Ollama specific portions if this is a llama request\n",
    "    if model_type == \"ollama\":\n",
    "        payload[\"prompt\"] = prompt\n",
    "        payload[\"model\"] = model_name\n",
    "        payload[\"options\"]= {\n",
    "            \"seed\": seed,\n",
    "            \"temperature\": temperature,\n",
    "            \"top_p\": top_p,\n",
    "            \"top_k\": top_k,\n",
    "            \"num_predict\": num_output_tokens\n",
    "        }\n",
    "\n",
    "        payload[\"format\"] = {\n",
    "            'properties': {\n",
    "                'score': {\n",
    "                    'enum': [1, 2, 3, 4, 5],\n",
    "                    'title': 'Score',\n",
    "                    'type': 'integer'\n",
    "                }\n",
    "            },\n",
    "            'required': ['score'],\n",
    "            'title': 'score',\n",
    "            'type': 'object'\n",
    "        }\n",
    "\n",
    "    # Create the Azure OpenAI specific portions if this is a GPT request\n",
    "    else:\n",
    "        payload[\"messages\"] = (\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": prompt[0]\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt[1]\n",
    "            }\n",
    "        )\n",
    "        payload[\"seed\"] = seed\n",
    "        payload[\"temperature\"] = temperature\n",
    "        payload[\"top_p\"] = top_p\n",
    "        payload[\"max_tokens\"] = num_output_tokens\n",
    "\n",
    "        payload[\"response_format\"] = {\n",
    "            \"type\": \"json_schema\",\n",
    "            \"json_schema\": {\n",
    "                \"name\": \"score\",\n",
    "                \"schema\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"score\": {\n",
    "                            \"enum\": [1, 2, 3, 4, 5],\n",
    "                            \"type\": \"integer\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"score\"],\n",
    "                    \"additionalProperties\": False\n",
    "                },\n",
    "                \"strict\": True\n",
    "            }\n",
    "        }\n",
    "\n",
    "    return payload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scoring Metric\n",
    "\n",
    "Once all summaries are scored, the correlation between those scores and the human evaluations is measured. Correlation is measured using Pearson, Spearman, and Kendall-Tau correlation. Pearson correlation measure the linear relationship between the machine scores and human evaluations while Spearman and Kendall-Tau look more so at the ranking of the data and compare the monotonic relationship between the machine scores and human evaluations. Kendall-Tau is typically more robust when dealing with tied ranks and will be the preferred metric of this analysis since both the machine scores and human evaluations have many ties.\n",
    "\n",
    "On the subject of tied rankings: G-Eval outputs a binned score - an integer with values 1, 2, 3, 4, 5 (although it never gave a score of 1 in any of my trials). In practice, this leads to ~500-way ties in the ranking of article-summary pairs. Allowing for so many ties makes the evaluation task significantly easier. The authors of G-Eval noted this fact, and they also confessed it likely explains why the normalized probaility score (which was a continuous score from 1-5) was worse than the binned integer score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation between the model and human evaluations\n",
    "def summEval_correlations(y_true: ArrayLike, y_pred: ArrayLike)-> Tuple[float, float, float]:\n",
    "    pearson = pearsonr(y_true, y_pred)[0]\n",
    "    spearman = spearmanr(y_true, y_pred)[0]\n",
    "    kendall_tau = kendalltau(y_true, y_pred)[0]\n",
    "    return pearson, spearman, kendall_tau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### G-Eval Prompting\n",
    "\n",
    "A G-Eval-based defines a metric in the prompt then instructs a non-fine-tunedl large language model to return the score of the article-summary pair using that metric. The model is instructed to (and forced by the structured outputs API) to return a JSON of the score. Enforcing a JSON output ensures that the score will be able to be parsed.\n",
    "\n",
    "The conversation that I will be simulating with stakeholders in regards to G-Eval revolves arount inconsistency. In this case, inconsistency refers to the LLM's outputs. When prompt engineering, a seemingly inconsequential change to the prompt or article-summary pair can lead to undesired outputs from the LLM. To highlight this, I include 3 extremely similar prompts and an additional similar prompt that is from Microsoft Prompt Flow [4]. Ideally, for a given model the 4 prompts should produce simlar results. In practice the 4 prompts produce different results with +/- 5 point (10% of the correlation scores), and the ranking of the prompts based on correlation score is different for each model.\n",
    "\n",
    "The prompts do not include any advanced prompt engineering techniques such as Chain of Thought, Few-Shot Prompting, etc. Different models respond differently to advanced prompt engineering techniques, and aren't included as this is a simple demonstration. Additionally, it can be difficult to prompt engineer when using a G-Eval-based prompt because the model output has no explainability. Without fine-tuning the model, it can be very difficult to develop confidence that the model's output will produce good results on new data.\n",
    "\n",
    "Prompt A is the base prompt. It is the most simple prompt.\n",
    "Prompt B is prompt A with an additional, redundant line that repeats part of the instructions.\n",
    "Prompt C is prompt B with several additional instructions. It instructs the model to be strict in its output. Notably, the model is well constrained as I use the structured outputs parameters with both Ollama and Azure OpenAI.\n",
    "Prompt D is similar to A, B, and C but is the most dissimlar of the 4. It is the Microsoft Prompt Flow example. It includes many short sentences, a short list of instructions, and generous newline characters.\n",
    "\n",
    "In fairness to LLMs, larger models tend to give more consistent outputs. GPT-4o and o1 tend to be much more consistent, but still not nearly as consistent as other non-LLM models and techniques. I elaborate more in the \"Discussion\" section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_eval_base_system_prompt = \"\"\"You are a super intelligent artificial intelligence that scores a summary based on its consistency with the source article. Your score is a rating from 1 to 5 (1, 2, 3, 4, 5) with 1 being a bad score and 5 being a good score. Your output is in the following format: {\"score\": rating} where rating is 1, 2, 3, 4 or 5.\n",
    "Consistency is defined as the factual alignment between the summary and the source article. A factually consistent summary contains only statements that are entailed by the source article. Summaries that contain inconsistent information should be penalized.\n",
    "{additional_instructions}\n",
    "\"\"\"\n",
    "\n",
    "g_eval_base_user_message = \"\"\"Source Article:\n",
    "{article}\n",
    "\n",
    "Summary:\n",
    "{summary}\"\"\"\n",
    "\n",
    "g_eval_consistency_suffix = \"\"\"\n",
    "\n",
    "Consistency JSON:\n",
    "\"\"\"\n",
    "\n",
    "def g_eval_faithfulness_prompt_a(\n",
    "        summary: str,\n",
    "        article: str,\n",
    "        model_type: llm_utils.ServerTypes\n",
    ") -> Union[str, Tuple[str, str]]:\n",
    "    system_prompt = g_eval_base_system_prompt\n",
    "    system_prompt = system_prompt.replace(\"{additional_instructions}\", \"\")\n",
    "    \n",
    "    user_message = g_eval_base_user_message\n",
    "    user_message = user_message.replace(\"{article}\", article)\n",
    "    user_message = user_message.replace(\"{summary}\", summary)\n",
    "    \n",
    "    if model_type == \"ollama\":\n",
    "        system_prompt = system_prompt + user_message + g_eval_consistency_suffix\n",
    "        return system_prompt\n",
    "\n",
    "    else:\n",
    "        return system_prompt, user_message\n",
    "\n",
    "\n",
    "\n",
    "def g_eval_faithfulness_prompt_b(\n",
    "        summary: str,\n",
    "        article: str,\n",
    "        model_type: llm_utils.ServerTypes\n",
    ") -> Union[str, Tuple[str, str]]:\n",
    "    additional_instructions = \"You will be given a summary and the source article. Your task is to answer with the rated score for that summary.\\n\"\n",
    "    system_prompt = g_eval_base_system_prompt\n",
    "    system_prompt = system_prompt.replace(\"{additional_instructions}\", additional_instructions)\n",
    "    \n",
    "    user_message = g_eval_base_user_message\n",
    "    user_message = user_message.replace(\"{article}\", article)\n",
    "    user_message = user_message.replace(\"{summary}\", summary)\n",
    "    \n",
    "    if model_type == \"ollama\":\n",
    "        system_prompt = system_prompt + user_message + g_eval_consistency_suffix\n",
    "        return system_prompt\n",
    "\n",
    "    else:\n",
    "        return system_prompt, user_message\n",
    "\n",
    "\n",
    "def g_eval_faithfulness_prompt_c(\n",
    "        summary: str,\n",
    "        article: str,\n",
    "        model_type: llm_utils.ServerTypes\n",
    ") -> Union[str, Tuple[str, str]]:\n",
    "    additional_instructions = g_eval_base_system_prompt + \"\"\"You will be given a summary and the source article. Your task is to answer with the rated score, and only the score, for that summary.\n",
    "Your answers should *strictly* be a number from 1 to 5. Do *not* output anything except a single number between 1 and 5.\n",
    "Do *not* output additional information, comments, or context. Do *not* answer with any spaces, whitespace, newline characters or any other formatting.\n",
    "\"\"\"\n",
    "    system_prompt = g_eval_base_system_prompt\n",
    "    system_prompt = system_prompt.replace(\"{additional_instructions}\", additional_instructions)\n",
    "    \n",
    "    user_message = g_eval_base_user_message\n",
    "    user_message = user_message.replace(\"{article}\", article)\n",
    "    user_message = user_message.replace(\"{summary}\", summary)\n",
    "    \n",
    "    if model_type == \"ollama\":\n",
    "        system_prompt = system_prompt + user_message + g_eval_consistency_suffix\n",
    "        return system_prompt\n",
    "\n",
    "    else:\n",
    "        return system_prompt, user_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g_eval_faithfulness_prompt_d(\n",
    "        summary: str,\n",
    "        article: str,\n",
    "        model_type: llm_utils.ServerTypes\n",
    ") -> Union[str, Tuple[str, str]]:\n",
    "    system_prompt = \"\"\"You will be given a source document. You will then be given one summary written for this source document.\n",
    "\n",
    "Your task is to rate the summary on one metric.\n",
    "\n",
    "Please make sure you read and understand these instructions carefully. Please keep this document open while reviewing, and refer to it as needed.\n",
    "\n",
    "Evaluation Criteria:\n",
    "\n",
    "Consistency (1-5) - the factual alignment between the summary and the summarized source. A factually consistent summary contains only statements that are entailed by the source document. Annotators were also asked to penalize summaries that contained hallucinated facts.\n",
    "\n",
    "Evaluation Steps:\n",
    "\n",
    "1. Read the source document carefully and identify the main facts and details it presents.\n",
    "2. Read the summary and compare it to the source document. Check if the summary contains any factual errors that are not supported by the source document.\n",
    "3. Assign a score for consistency based on the Evaluation Criteria.\n",
    "4. Format your response into a JSON of the form {\"score\": consistency} where consistency is your score from 1 to 5.\n",
    "\"\"\"\n",
    "\n",
    "    user_message = f\"\"\"Source Document:\n",
    "\n",
    "{article}\n",
    "\n",
    "Summary:\n",
    "\n",
    "{summary}\"\"\"\n",
    "\n",
    "    if model_type == \"ollama\":\n",
    "        system_prompt = system_prompt + \"\\n\" + user_message + g_eval_consistency_suffix + \"\\n\"\n",
    "        return system_prompt\n",
    "\n",
    "    else:\n",
    "        return system_prompt, user_message\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calling the Models\n",
    "\n",
    "We have 1600 pairs of summaries and articles to churn through. While it is overkill for my local Ollama server and GPT deployment with a low token per minute rate limit, I make these calls asyncronously. It is overkill because I then have to limit the number of simultaneous connections to Ollama and GPT otherwise they may timeout waiting in the queue. For GPT, I use exponential backoff to throttle the calls when rate limited is reached.\n",
    "\n",
    "As mentioned briefly in the \"G-Eval Prompting\" section, a JSON schema is enforced on the model output to ensure the output will be able to be parsed. Without it, the model can \"go rogue\" and might output more than just the integer answer that we want. The strucutred outputs API puts more guardrails on the model and makes them more acceptable for production environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def g_eval_faithfulness(\n",
    "        model_name: llm_utils.AllowedModelNames,\n",
    "        session: aiohttp.ClientSession,\n",
    "        summary: str,\n",
    "        article: str,\n",
    "        prompt_base: Callable\n",
    "        ):\n",
    "    \"\"\" Returns the score for one article-summary pair using the given model and prompt_function\n",
    "    \"\"\"\n",
    "    model_type = LlmUtils.get_model_type(model_name)\n",
    "    prompt = prompt_base(summary, article, model_type)\n",
    "    response = await LlmUtils.call_cached_llm(\n",
    "        model_name,\n",
    "        session,\n",
    "        get_request_payload(\n",
    "            model_name,\n",
    "            prompt,\n",
    "        ),\n",
    "        CACHE,\n",
    "        returned_stopped_only=False\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        return int(json.loads(response.strip())['score'])\n",
    "    except:\n",
    "        print(\"response:\", response)\n",
    "        return None\n",
    "\n",
    "\n",
    "async def g_eval_faithfulnesses(\n",
    "        model_name: llm_utils.AllowedModelNames,\n",
    "        summ_eval: pd.DataFrame,\n",
    "        prompts: Dict[str, Callable]\n",
    "):\n",
    "    \"\"\" Returns the correlation for the given model on all article-summary pairs for each of the 4 prompts\n",
    "    \"\"\"\n",
    "    # Setup the processing for this model\n",
    "    model_type = LlmUtils.get_model_type(model_name)\n",
    "    num_conns = NUM_PARALLEL_CONNS_LLAMA if model_type == \"ollama\" else NUM_PARALLEL_CONNS_GPT\n",
    "    connector = aiohttp.TCPConnector(limit=num_conns)\n",
    "    timeout = aiohttp.ClientTimeout(total=None)\n",
    "\n",
    "    geval_correlations = {}\n",
    "    async with aiohttp.ClientSession(connector=connector, timeout=timeout) as session:\n",
    "        # Run each prompt\n",
    "        for prompt_name, prompt_base in prompts.items():\n",
    "            # Asyncronously run each article-summary pair through the model\n",
    "            geval_tasks = [\n",
    "                g_eval_faithfulness(model_name, session, summary, article, prompt_base)\n",
    "                for summary, article in zip(summ_eval[\"summary\"], summ_eval[\"article\"])\n",
    "            ]\n",
    "            geval_results = await asyncio.gather(*geval_tasks)\n",
    "            \n",
    "            # Get the correlation scors for this model and prompt\n",
    "            geval_correlations[prompt_name] = summEval_correlations(summ_eval[\"consistency\"], geval_results)\n",
    "            \n",
    "            with open(CACHE_PATH, \"wb\") as f:\n",
    "                pickle.dump(CACHE, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    return geval_correlations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = {\n",
    "    \"a\": g_eval_faithfulness_prompt_a,\n",
    "    \"b\": g_eval_faithfulness_prompt_b,\n",
    "    \"c\": g_eval_faithfulness_prompt_c,\n",
    "    \"d\": g_eval_faithfulness_prompt_d\n",
    "}\n",
    "g_eval_correlations = {\n",
    "    \"phi4\": await g_eval_faithfulnesses(\"phi4\", summ_eval, prompts),\n",
    "    \"llama3.2-vision\": await g_eval_faithfulnesses(\"llama3.2-vision\", summ_eval, prompts),\n",
    "    \"deepseek-r1:14b\": await g_eval_faithfulnesses(\"deepseek-r1:14b\", summ_eval, prompts),\n",
    "    # \"gpt4o-mini\": await g_eval_faithfulnesses(\"gpt4o-mini\", summ_eval, prompts),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./llm_cache.pickle\", \"wb\") as f:\n",
    "    pickle.dump(CACHE, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance\n",
    "\n",
    "The models perform well enough, but the scores have a good amount of variation. Not only were the scores +/- 5 points (10% of the score), but the rankings of the prompts differed between models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = list(g_eval_correlations.keys())\n",
    "prompts = list(g_eval_correlations[models[0]].keys())\n",
    "correlation_names = ['Pearson', 'Spearman', 'Kendall-Tau']\n",
    "\n",
    "x = np.arange(len(models))\n",
    "width = 0.2  # width of bars\n",
    "\n",
    "fig, axes = plt.subplots(1, len(correlation_names), figsize=(25, 5), sharey=True)\n",
    "handles = []\n",
    "labels = [f'Prompt {prompt}' for prompt in prompts]\n",
    "\n",
    "for i, metric in enumerate(correlation_names):\n",
    "    ax = axes[i]\n",
    "    for j, prompt in enumerate(prompts):\n",
    "        values = [g_eval_correlations[model][prompt][i] for model in models]\n",
    "        bars = ax.bar(x + j * width, values, width)\n",
    "        \n",
    "        # Collect handles and labels only once\n",
    "        if i == 0:\n",
    "            handles.append(bars[0])\n",
    "            ax.set_ylabel(\"Score\")\n",
    "    \n",
    "    ax.set_xlabel(\"Models\")\n",
    "    ax.set_title(f\"{metric} Correlation\")\n",
    "    ax.set_xticks(x + width * 1.5)\n",
    "    ax.set_xticklabels(models)\n",
    "\n",
    "# Add a single legend to the figure\n",
    "fig.legend(handles, labels, loc='lower center', ncol=len(prompts), bbox_to_anchor=(0.5, -0.1))\n",
    "plt.suptitle(\"Evaluation Scores by Model, Technique, and Correlation Type\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion and Example Application\n",
    "\n",
    "Hopefully I have shown that LLMs are not a silver bullet and there are concerns in using an LLMs for tasks that is not generative in nature. A task such as evaluating the faithfulness of article-summary pairs is likely best to be left to an embedding model rather than a language model.\n",
    "\n",
    "That being said, generative models absolutely have their place in the data scientists or machine learning engineer's toolkit. For one, generative models are often able to very quickly produce a good enough result. Consider the following example, keeping in line with the spirit of the summary-article pairs of SummEval:\n",
    "* Let's assume we are working on an internal process to summarizes full length transcripts of video calls\n",
    "* Previously, it was company policy for employees to manually summarize meetings\n",
    "* The full transcript along with the summary is logged in some records system\n",
    "* For these summaries, there are company guidelines as far as professional language, factual correctness, etc\n",
    "* These summaries are very important for legal and record keeping reasons\n",
    "* Employees are inconsistent with their summaries. Many employees don't do a great job writing summaries, and we want to improve the summaries\n",
    "* Instead of manually reviewing every summary-transcript pair, we can use a technique like G-Eval to score them based on those existing policies\n",
    "* Any transcript-summary pair with a score of 3 or lower gets sent back to a human to be revised\n",
    "\n",
    "Better yet, we can leverage the generative nature of LLMs to create the summaries from the start\n",
    "* Have the LLM create the summary based on the full transcript. But feedback from the stakeholders indicates the biggest issue with the summaries is the faithfulness.\n",
    "* Use a G-Eval-based technique to evaluate the transcript-summary pair, sending it back to the LLM to be rewritten if it scores a 3 or lower\n",
    "* After a summary with a score of 4 or 5 is created, we present it to a human employee for the final evaluation\n",
    "* After the human employee reviews the summary and approves it, it is sent to the same records system as before\n",
    "\n",
    "The best part about this system was that it could be created quickly. It required no training data and the infrastructure to host the LLM already exists via Azure OpenAI Service or AWS Bedrock. The other parts of this service, like how we show it to the human for review or how we automatically log it in the record system, are necessary regardless of the machine learning technique we use to summarize and evalute the summaries. Even if this system isn't perfect, it's a lot easier to correct a factually inconsistent summary than it is to write one from scratch. So we are likely still reducing the manual labor required to write the summaries even if we didn't eliminate it.\n",
    "\n",
    "There is a major benefit to quickly creating a system: \"A lot of times, people don't know what they want until you show it to them” - Steve Jobs. Stakeholders (and people in general) often know what improvements can be made, and what would drive business value. However, they don't know how to create that system. In our example, the stakeholders knew they needed to improve the summarization, but didn't know how they could go about doing it. Create a system, even one that is imperfect like G-Eval, closes tha gap between a paper diagram and the real world. After creating the system, our stakeholders can re-evaluate if we solved the problem at hand. We have a modular system, so if the summaries are not faithful enough to thir source transcript due to hallucinations, we can improve the summarization and evaluation portions of the pipeline.\n",
    "\n",
    "If we wanted to improve the evaluation step of the pipeline, MQAG [9] and TrueTeacher [10] are both techniques that are more correlated with human evaluation of faithfulness. MQAGs especially so.\n",
    "* MQAG uses a language model trained on multiple choice questions and answers, and an embedding model is trained to answer those questions. The language model is then used to generate multiple choice questions and answers from the summary. The embedding model is trained to answers those questions based on the source article (without access to the summary). If the embedding model gets the answer incorrect, that implies information in the summary differs from information in the article (and therefore the summary is to some degree unfaithful to the article). Based on the proportion of correct answers, the overall faithfulness of the summary can be calculated.\n",
    "* TrueTeacher first generates machine written summaries from source articles. A LLM, pre-trained on similar text, is then used to label those articles with faithfulness labels. Because the summaries are machine written, they are expected to have some amount of factual inconsistencies. An embedding model is then trained on the source articles and generated summaries with the labels as ground truth. The embedding model is eventually able to surpase the performance of the LLM, even though the LLM was used to generate the labels in the first place.\n",
    "\n",
    "TODO: The drawback to those methods is they require some kind of training data, and more time to develop the model as well as host it. MQAG requires a set of questions and answers generated on text similar to the application. TrueTeacher requires a large corpus for pretraining an LLM, even if it isn't labeled. G-Eval requires nothing and can be used with a good degree of success out of the box. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Citations\n",
    "\n",
    "[1] Fabbri, Alexander R., et al. \"Summeval: Re-evaluating summarization evaluation.\" Transactions of the Association for Computational Linguistics 9 (2021): 391-409.\n",
    "\n",
    "[2] Liu, Yang, et al. \"G-eval: Nlg evaluation using gpt-4 with better human alignment.\" arXiv preprint arXiv:2303.16634 (2023).\n",
    "\n",
    "[3] Maynez, Joshua, et al. \"On faithfulness and factuality in abstractive summarization.\" arXiv preprint arXiv:2005.00661 (2020).\n",
    "\n",
    "[4] Fujimoto, K., & Maneck, B. (2024, February 24). Prompt flow. Prompt flow - Prompt flow documentation. https://microsoft.github.io/promptflow/ Maneck, B., & Fujimoto, K. (n.d.). Promptflow/examples/flows/evaluation/eval-summarization at main · Microsoft/promptflow. Summarization Evaluation. https://github.com/microsoft/promptflow/tree/main/examples/flows/evaluation/eval-summarization#meta-evaluation \n",
    "\n",
    "[5] AI@Meta. (2024). Llama 3.2 Model Card. GitHub. https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD.md \n",
    "\n",
    "[6] Abdin, Marah, et al. \"Phi-4 technical report.\" arXiv preprint arXiv:2412.08905 (2024).\n",
    "\n",
    "[7] DeepSeek-AI, et al. ‘DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning’. arXiv [Cs.CL], 2025, http://arxiv.org/abs/2501.12948. arXiv.\n",
    "\n",
    "[8] GPT-4O Mini: Advancing Cost-Efficient Intelligence. GPT-4o mini: advancing cost-efficient intelligence. (2024, July 18). https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence \n",
    "\n",
    "[9] Manakul, Potsawee, Adian Liusie, and Mark JF Gales. \"MQAG: Multiple-choice question answering and generation for assessing information consistency in summarization.\" arXiv preprint arXiv:2301.12307 (2023).\n",
    "\n",
    "[10] Gekhman, Zorik, et al. \"Trueteacher: Learning factual consistency evaluation with large language models.\" arXiv preprint arXiv:2305.11171 (2023).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
