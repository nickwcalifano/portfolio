{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Automatic Summary Evaluation\n",
    "\n",
    "This project demonstrates an LLM-as-a-judge technique to evaluate summaries generated by machine learning models. Summary evaluation is a useful application of LLMs in production environments and has analogies in similar evaluations of GenAI outputs (or the automated evaluation of human outputs).\n",
    "\n",
    "This notebook uses data from SummEval [1] which is a dataset of CNN and Daily Mail articles, machine generated summaries of those articles, and human evaluations of those summaries. In this notebook, a G-Eval technique is used to evaluate the summaries on consistency. G-Eval achieves relatively high correlation with human evaluations with the benefit that it can be used with a foundational model and does not require fine-tuning to achieve that relatively high correlation.\n",
    "\n",
    "G-Eval [2] was originally used to better measure Relevance, Consistency, Fluency, and Coherence. Relevance being the measure of how well the summary captures the key points of the article. Consistency being the measure of whether the summary reproduces all facts accurately and does not make up untrue information. Fluency being the measure of the quality of individual sentences (grammatically correct). Coherence being the measure of the quality of the summary as a whole.\n",
    "\n",
    "In general, machine written summaries are highly Fluent and Coherent [3] - to the point that focus as can shift entirely to Relevance and Consistency. Relevance is not as \"solved\" as Fluency or Coherence, but LLMs often produce output with acceptable levels of relevance. The biggest struggle for LLMs is often consistency - coming from an incorrect representation of events in the source text. These incorrect representations are known as hallucinations and can be an incorrect description of events that contradict the source (known as intrinsic hallucinations), or the introduction of new information that cannot be verified by the source (extrinsic hallucinations). that being said, SummEval was published in 2020 and the summaries of the dataset were produced with non-LLM models and are therefore not always fully fluent or coherent.\n",
    "\n",
    "This implementation of G-Eval uses several custom prompts as well as a modified version of an example prompt from Microsoft's Prompt Flow [4]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import Union, Tuple, Dict, Callable\n",
    "\n",
    "from numpy.typing import ArrayLike\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr, spearmanr, kendalltau\n",
    "\n",
    "import llm_utils\n",
    "from llm_utils import LlmUtils\n",
    "\n",
    "# Dataset paths\n",
    "SUMM_EVAL_DATA = \"./data/summEval/summeval.json\"\n",
    "\n",
    "# Cached request/responses\n",
    "CACHE_PATH = \"./data/summEval/llm_cache.pkl\"\n",
    "\n",
    "# Async Parameters\n",
    "NUM_PARALLEL_CONNS_LLAMA = 3\n",
    "NUM_PARALLEL_CONNS_GPT = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Dataset\n",
    "\n",
    "This dataset is from SummEval and is comprised of full text (source) articles from CNN and Daily Mail, summaries generated by various NLP models, and human evaluations of those summaries.\n",
    "\n",
    "For this notebook, the only score we are interested in is the consistency score. Consistency can have multiple definitions in the literature and in practice, but the definition given to the human evaluators for SummEval was: \"The [Consistency] rating measures whether the facts in the summary are consistent with the facts in the original article. Consider whether the summary does reproduce all facts accurately and does not make up untrue information.\"\n",
    "\n",
    "It has been several years since SummEval has been published. Since then, much of the literature has broken consistency into 2 metrics: faithfulness (also known as groundedness) and coverage (also known as completeness or recall). This notebook will focus on a generic consistency since there is not ground truth data for faithfulness or coverage in the SummEval dataset.\n",
    "\n",
    "There is an interesting, almost philosophical, discussion to be had about faithfulness and coverage. Domain specific language, especially with easy to understand definitions, can often be useful when discussing with stakeholders because it can avoid miscommunication. Faithfulness is defined as staying consistent and truthful to the provided source, and we do not consider ‘factuality’ where valid external facts are acceptable [3]. A summary that omits details but isn't otherwise misleading or inaccurate would still be faithful. Not considering factuality means a factually inaccurate piece of information in the source should be faithfully represented in the summary. Therefore, a faithful summary of a flat-earther article would have the same factual inaccuracies that are present in the source article. Coverage is defined as the comprehensiveness of the summary and quantifies how well a summary captures and accurately represents key information from the source. A summary that contains all information from the text, but hallucinates new information, so long as the hallucinations do not mislead or inaccurately represent the source, would still have high coverage.\n",
    "\n",
    "If a stakeholder says a summary is unacceptable due to low factual accuracy, that may mean a number of things - the model is making up information (low faithfulness - external hallucinations), the model is confusing information like who-did-what (low faithfulness - intrinsic information), the model is missing information (low coverage), etc. It is important to discuss and find patterns in the output, and having appropriate language goes a long way to fill in communication gaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data used for evaluation available from the G-Eval paper's GitHub repo\n",
    "#   https://raw.githubusercontent.com/nlpyang/geval/refs/heads/main/data/summeval.json\n",
    "summ_eval = json.load(open(SUMM_EVAL_DATA))\n",
    "summ_eval = pd.DataFrame(summ_eval)\n",
    "summ_eval[\"consistency\"] = summ_eval[\"scores\"].apply(lambda x: x[\"consistency\"])\n",
    "summ_eval.drop([\"doc_id\", \"system_id\", \"reference\", \"scores\"] , axis=1, inplace=True)\n",
    "summ_eval.rename(columns={\"source\": \"article\", \"system_output\": \"summary\"}, inplace=True)\n",
    "\n",
    "# At the time of publication, 5 articles (and all 16 summaries associated with each of the articles) get flagged by the\n",
    "#   Azure OpenAI Service's Content Filter, even on the most permissive settings\n",
    "# This filter can be removed for trusted users so long as they still uphold Microsoft's content moderation policies\n",
    "#   My personal account is not a trusted user, so I cannot remove the content filter\n",
    "# I enumerate each index explicitly because it would be possible for a specific summary to be flagged, but the other\n",
    "#   summaries associated with the same article. This makes it easier to apply to a new dataset in the future\n",
    "# Even though I could process these with the Ollama models, I am just going to ignore them for simplicity\n",
    "azure_error_indices = [\n",
    "      80,   81,   82,   83,   84,   85,   86,   87,   88,   89,   90,   91,   92,   93,   94,   95,\n",
    "     352,  353,  354,  355,  356,  357,  358,  359,  360,  361,  362,  363,  364,  365,  366,  367,\n",
    "     448,  449,  450,  451,  452,  453,  454,  455,  456,  457,  458,  459,  460,  461,  462,  463,\n",
    "    1472, 1473, 1474, 1475, 1476, 1477, 1478, 1479, 1480, 1481, 1482, 1483, 1484, 1485, 1486, 1487,\n",
    "    1520, 1521, 1522, 1523, 1524, 1525, 1526, 1527, 1528, 1529, 1530, 1531, 1532, 1533, 1534, 1535\n",
    "]\n",
    "summ_eval.drop(azure_error_indices, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calling the models\n",
    "\n",
    "This notebook compares the performance of 4 models on 4 prompts.\n",
    "\n",
    "The models are Llama3.2-vision [5], Phi4 [6], DeepSeek-R1-Distill-Qwen-14B [7], and GPT-4o mini [8]. \n",
    "* Llama3.2-vision is an 11B parameter model released by Meta in 09/2024\n",
    "* Phi4 is a 14B parameter model released by Microsoft in 12/2024\n",
    "* DeepSeek-R1-Distill-Qwen-14B uses the Qwen-14B model architecture and was distilled from DeepSeek-R1 (released by DeepSeek in 01/2025)\n",
    "* GPT 4o mini is OpenAI's \"most cost-efficient small model\" and was released in 07/2024\n",
    "\n",
    "All models except GPT-4o mini are open source and are called using a local OLLAMA server. GPT-4o mini is called using the Azure OpenAI Service. It would be easy to compare more models, especially ones available on Ollama or Azure OpenAI Service. The open-source models were selected because they are the most performative (and largest) models that I can reasonably run on my personal GPU. GPT-4o mini was selected because it is the most performative model for which I am willing to pay the monetary cost. GPT-4o and o1-mini are both more than an order of magnitude more expensive and o1 is nearly 2 orders of magnitude more expensive.\n",
    "\n",
    "Both OLLAMA and the Azure OpenAI Service use similar REST API endpoints, but there are some small differences between the APIs. All differences are contained and managed by llm_utils."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the calls to the LLMs get saved to a 'cache', just a dictionary with requests as keys and responses as values\n",
    "# This is useful when rerunning and adding new models or otherwise experimenting or debugging since the calls to the\n",
    "#   LLMs are expensive both in time and for GPT, money. \n",
    "\n",
    "# I'm using a try-except in this manner so I can re-run the full notebook without accidentally overwriting the cache.\n",
    "# The cache is pickled and saved to disk once all samples are passed through a prompt-model pair\n",
    "try:\n",
    "    CACHE\n",
    "except:\n",
    "    if os.path.exists(CACHE_PATH):\n",
    "        with open(CACHE_PATH, \"rb\") as f:\n",
    "            CACHE = pickle.load(f)\n",
    "    else:\n",
    "        CACHE = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the request payload for the model calls\n",
    "# Uses structured outputs to ensure the model gives a output than can be parsed\n",
    "def get_request_payload(\n",
    "        model_name: llm_utils.AllowedModelNames,\n",
    "        prompt: Union[str, Tuple[str, str]]\n",
    ") -> dict:\n",
    "    \"\"\" Creates the request payload for the given model\n",
    "    \"\"\"\n",
    "    # Set some fields based on the inputs\n",
    "    model_type = LlmUtils.get_model_type(model_name)\n",
    "    num_output_tokens = 10\n",
    "    seed = 314159265\n",
    "\n",
    "    # If randomness options were not supplied, set them to minimum values\n",
    "    temperature = 0.0\n",
    "    top_p = 0.0\n",
    "    top_k = 1\n",
    "\n",
    "    # Create the portions of the request payload that are identical between Ollama and Azure OpenAI\n",
    "    payload = {\n",
    "        \"stream\": False,\n",
    "    }\n",
    "\n",
    "    # Create the Ollama specific portions if this is a llama request\n",
    "    if model_type == \"ollama\":\n",
    "        payload[\"prompt\"] = prompt\n",
    "        payload[\"model\"] = model_name\n",
    "        payload[\"options\"]= {\n",
    "            \"seed\": seed,\n",
    "            \"temperature\": temperature,\n",
    "            \"top_p\": top_p,\n",
    "            \"top_k\": top_k,\n",
    "            \"num_predict\": num_output_tokens\n",
    "        }\n",
    "\n",
    "        payload[\"format\"] = {\n",
    "            'properties': {\n",
    "                'score': {\n",
    "                    'enum': [1, 2, 3, 4, 5],\n",
    "                    'title': 'Score',\n",
    "                    'type': 'integer'\n",
    "                }\n",
    "            },\n",
    "            'required': ['score'],\n",
    "            'title': 'score',\n",
    "            'type': 'object'\n",
    "        }\n",
    "\n",
    "    # Create the Azure OpenAI specific portions if this is a GPT request\n",
    "    else:\n",
    "        payload[\"messages\"] = (\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": prompt[0]\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt[1]\n",
    "            }\n",
    "        )\n",
    "        payload[\"seed\"] = seed\n",
    "        payload[\"temperature\"] = temperature\n",
    "        payload[\"top_p\"] = top_p\n",
    "        payload[\"max_tokens\"] = num_output_tokens\n",
    "\n",
    "        payload[\"response_format\"] = {\n",
    "            \"type\": \"json_schema\",\n",
    "            \"json_schema\": {\n",
    "                \"name\": \"score\",\n",
    "                \"schema\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"score\": {\n",
    "                            \"enum\": [1, 2, 3, 4, 5],\n",
    "                            \"type\": \"integer\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"score\"],\n",
    "                    \"additionalProperties\": False\n",
    "                },\n",
    "                \"strict\": True\n",
    "            }\n",
    "        }\n",
    "\n",
    "    return payload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scoring Metric\n",
    "\n",
    "Once all summaries are scored, the correlation between those scores and the human evaluations is measured. Correlation is measured using Pearson, Spearman, and Kendall-Tau correlation. Pearson correlation measure the linear relationship between the machine scores and human evaluations while Spearman and Kendall-Tau look more so at the ranking of the data and compare the monotonic relationship between the machine scores and human evaluations. All 3 metrics will likely be directionally correct, but Kendall-Tau is typically more robust when dealing with tied ranks and would be the preferred metric if we were trying to find the best model/prompt.\n",
    "\n",
    "On the subject of tied rankings: G-Eval outputs a binned score - an integer with values 1, 2, 3, 4, 5. In practice, this leads to ~500-way ties in the ranking of article-summary pairs. Allowing for so many ties makes the evaluation task significantly easier. The authors of G-Eval noted this fact, and they also confessed it likely explains why the normalized probability score (which was a continuous score from 1-5) was worse than the binned integer score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation between the model and human evaluations\n",
    "def summEval_correlations(y_true: ArrayLike, y_pred: ArrayLike)-> Tuple[float, float, float]:\n",
    "    pearson = pearsonr(y_true, y_pred)[0]\n",
    "    spearman = spearmanr(y_true, y_pred)[0]\n",
    "    kendall_tau = kendalltau(y_true, y_pred)[0]\n",
    "    return pearson, spearman, kendall_tau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### G-Eval Prompting\n",
    "\n",
    "A G-Eval-based defines a metric in the prompt then instructs a non-fine-tuned large language model to return the score using that metric given an example. In our case, the metric is consistency and the example is a summary-article pair. In my implementation, the model is instructed to (and forced by the structured outputs API) to return a JSON of the score. Enforcing a JSON output ensures that the score will be able to be parsed.\n",
    "\n",
    "The prompts do not include any advanced prompt engineering techniques such as Chain of Thought, Few-Shot Prompting, etc. Different models respond differently to advanced prompt engineering techniques, and aren't included as this is a simple demonstration. \n",
    "\n",
    "The prompts are:\n",
    "* Prompt A is the base prompt. It is the most simple prompt.\n",
    "* Prompt B is prompt A with an additional, redundant line that repeats part of the instructions.\n",
    "* Prompt C is prompt B with several additional instructions. It instructs the model to be strict in its output. Notably, the model is well constrained as I use the structured outputs parameters with both Ollama and Azure OpenAI. So instructing it not to output only the score and no newline characters doesn't seem productive. (But apparently is productive because it was the best prompt for all models)\n",
    "* Prompt D is the Microsoft Prompt Flow example. It is still similar to A, B, and C but is the most dissimilar of the 4. It includes many short sentences, a short list of instructions, and generous newline characters.\n",
    "\n",
    "The conversation that I will be simulating with stakeholders in regard to G-Eval revolves around inconsistency. In this case, \"inconsistency\" refers to the LLM generating a different result with a seemingly inconsequential change to the prompt or article-summary pair. To highlight this, I include 3 extremely similar prompts and an additional similar prompt that is from Microsoft Prompt Flow [4]. Ideally, for a given model the 4 prompts should produce similar results. In practice the 4 prompts produce different correlation scores with +/- 5% variation. We might have concerns deploying such a model to production because a small shift in distribution of the incoming text might cause a 5% decline in the model's performance. Without fine-tuning the model, it can be difficult to develop confidence that the model's output will produce good results on new data.\n",
    "\n",
    "In fairness to LLMs, larger models tend to give more consistent outputs. GPT-4o and o1 tend to be much more consistent, but still not nearly as consistent as other non-LLM models and techniques. I elaborate more in the \"Discussion\" section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_eval_base_system_prompt = \"\"\"You are a super intelligent artificial intelligence that scores a summary based on its consistency with the source article. Your score is a rating from 1 to 5 (1, 2, 3, 4, 5) with 1 being a bad score and 5 being a good score. Your output is in the following format: {\"score\": rating} where rating is 1, 2, 3, 4 or 5.\n",
    "Consistency is defined as the factual alignment between the summary and the source article. A factually consistent summary contains only statements that are entailed by the source article. Summaries that contain inconsistent information should be penalized.\n",
    "{additional_instructions}\n",
    "\"\"\"\n",
    "\n",
    "g_eval_base_user_message = \"\"\"Source Article:\n",
    "{article}\n",
    "\n",
    "Summary:\n",
    "{summary}\"\"\"\n",
    "\n",
    "g_eval_consistency_suffix = \"\"\"\n",
    "\n",
    "Consistency JSON:\n",
    "\"\"\"\n",
    "\n",
    "def g_eval_faithfulness_prompt_a(\n",
    "        summary: str,\n",
    "        article: str,\n",
    "        model_type: llm_utils.ServerTypes\n",
    ") -> Union[str, Tuple[str, str]]:\n",
    "    system_prompt = g_eval_base_system_prompt\n",
    "    system_prompt = system_prompt.replace(\"{additional_instructions}\", \"\")\n",
    "    \n",
    "    user_message = g_eval_base_user_message\n",
    "    user_message = user_message.replace(\"{article}\", article)\n",
    "    user_message = user_message.replace(\"{summary}\", summary)\n",
    "    \n",
    "    if model_type == \"ollama\":\n",
    "        system_prompt = system_prompt + user_message + g_eval_consistency_suffix\n",
    "        return system_prompt\n",
    "\n",
    "    else:\n",
    "        return system_prompt, user_message\n",
    "\n",
    "\n",
    "\n",
    "def g_eval_faithfulness_prompt_b(\n",
    "        summary: str,\n",
    "        article: str,\n",
    "        model_type: llm_utils.ServerTypes\n",
    ") -> Union[str, Tuple[str, str]]:\n",
    "    additional_instructions = \"You will be given a summary and the source article. Your task is to answer with the rated score for that summary.\\n\"\n",
    "    system_prompt = g_eval_base_system_prompt\n",
    "    system_prompt = system_prompt.replace(\"{additional_instructions}\", additional_instructions)\n",
    "    \n",
    "    user_message = g_eval_base_user_message\n",
    "    user_message = user_message.replace(\"{article}\", article)\n",
    "    user_message = user_message.replace(\"{summary}\", summary)\n",
    "    \n",
    "    if model_type == \"ollama\":\n",
    "        system_prompt = system_prompt + user_message + g_eval_consistency_suffix\n",
    "        return system_prompt\n",
    "\n",
    "    else:\n",
    "        return system_prompt, user_message\n",
    "\n",
    "\n",
    "def g_eval_faithfulness_prompt_c(\n",
    "        summary: str,\n",
    "        article: str,\n",
    "        model_type: llm_utils.ServerTypes\n",
    ") -> Union[str, Tuple[str, str]]:\n",
    "    additional_instructions = g_eval_base_system_prompt + \"\"\"You will be given a summary and the source article. Your task is to answer with the rated score, and only the score, for that summary.\n",
    "Your answers should *strictly* be a number from 1 to 5. Do *not* output anything except a single number between 1 and 5.\n",
    "Do *not* output additional information, comments, or context. Do *not* answer with any spaces, whitespace, newline characters or any other formatting.\n",
    "\"\"\"\n",
    "    system_prompt = g_eval_base_system_prompt\n",
    "    system_prompt = system_prompt.replace(\"{additional_instructions}\", additional_instructions)\n",
    "    \n",
    "    user_message = g_eval_base_user_message\n",
    "    user_message = user_message.replace(\"{article}\", article)\n",
    "    user_message = user_message.replace(\"{summary}\", summary)\n",
    "    \n",
    "    if model_type == \"ollama\":\n",
    "        system_prompt = system_prompt + user_message + g_eval_consistency_suffix\n",
    "        return system_prompt\n",
    "\n",
    "    else:\n",
    "        return system_prompt, user_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g_eval_faithfulness_prompt_d(\n",
    "        summary: str,\n",
    "        article: str,\n",
    "        model_type: llm_utils.ServerTypes\n",
    ") -> Union[str, Tuple[str, str]]:\n",
    "    system_prompt = \"\"\"You will be given a source document. You will then be given one summary written for this source document.\n",
    "\n",
    "Your task is to rate the summary on one metric.\n",
    "\n",
    "Please make sure you read and understand these instructions carefully. Please keep this document open while reviewing, and refer to it as needed.\n",
    "\n",
    "Evaluation Criteria:\n",
    "\n",
    "Consistency (1-5) - the factual alignment between the summary and the summarized source. A factually consistent summary contains only statements that are entailed by the source document. Annotators were also asked to penalize summaries that contained hallucinated facts.\n",
    "\n",
    "Evaluation Steps:\n",
    "\n",
    "1. Read the source document carefully and identify the main facts and details it presents.\n",
    "2. Read the summary and compare it to the source document. Check if the summary contains any factual errors that are not supported by the source document.\n",
    "3. Assign a score for consistency based on the Evaluation Criteria.\n",
    "4. Format your response into a JSON of the form {\"score\": consistency} where consistency is your score from 1 to 5.\n",
    "\"\"\"\n",
    "\n",
    "    user_message = f\"\"\"Source Document:\n",
    "\n",
    "{article}\n",
    "\n",
    "Summary:\n",
    "\n",
    "{summary}\"\"\"\n",
    "\n",
    "    if model_type == \"ollama\":\n",
    "        system_prompt = system_prompt + \"\\n\" + user_message + g_eval_consistency_suffix + \"\\n\"\n",
    "        return system_prompt\n",
    "\n",
    "    else:\n",
    "        return system_prompt, user_message\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calling the Models\n",
    "\n",
    "We have ~1500 pairs of summaries and articles to churn through (several of the examples are removed due to triggering the Azure OpenAI Content Filter). I make these calls asynchronously, though that it is overkill for my local Ollama server and my GPT deployment with a low token per minute rate limit. It is overkill because I then have to limit the number of simultaneous connections to Ollama and GPT otherwise they may timeout waiting in the queue. For GPT, I use exponential backoff to throttle the calls when rate limited is reached.\n",
    "\n",
    "As mentioned briefly in the \"G-Eval Prompting\" section, a JSON schema is enforced on the model output to ensure the output will be able to be parsed. Without it, the model can \"go rogue\" and might output more than just the integer answer that we want. The structured outputs API puts more guardrails on the model and makes them more acceptable for production environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def g_eval_faithfulness(\n",
    "        model_name: llm_utils.AllowedModelNames,\n",
    "        session: aiohttp.ClientSession,\n",
    "        summary: str,\n",
    "        article: str,\n",
    "        prompt_base: Callable\n",
    "        ):\n",
    "    \"\"\" Returns the score for one article-summary pair using the given model and prompt_function\n",
    "    \"\"\"\n",
    "    model_type = LlmUtils.get_model_type(model_name)\n",
    "    prompt = prompt_base(summary, article, model_type)\n",
    "    response = await LlmUtils.cached_llm_infer(\n",
    "        model_name,\n",
    "        session,\n",
    "        get_request_payload(\n",
    "            model_name,\n",
    "            prompt,\n",
    "        ),\n",
    "        CACHE,\n",
    "        returned_stopped_only=False\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        return int(json.loads(response.strip())['score'])\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "async def g_eval_faithfulnesses(\n",
    "        model_name: llm_utils.AllowedModelNames,\n",
    "        summ_eval: pd.DataFrame,\n",
    "        prompts: Dict[str, Callable]\n",
    "):\n",
    "    \"\"\" Returns the correlation for the given model on all article-summary pairs for each of the 4 prompts\n",
    "    \"\"\"\n",
    "    # Setup the processing for this model\n",
    "    model_type = LlmUtils.get_model_type(model_name)\n",
    "    num_conns = NUM_PARALLEL_CONNS_LLAMA if model_type == \"ollama\" else NUM_PARALLEL_CONNS_GPT\n",
    "    connector = aiohttp.TCPConnector(limit=num_conns)\n",
    "    timeout = aiohttp.ClientTimeout(total=None)\n",
    "\n",
    "    geval_correlations = {}\n",
    "    async with aiohttp.ClientSession(connector=connector, timeout=timeout) as session:\n",
    "        # Run each prompt\n",
    "        for prompt_name, prompt_base in prompts.items():\n",
    "            # asynchronously run each article-summary pair through the model\n",
    "            geval_tasks = [\n",
    "                g_eval_faithfulness(model_name, session, summary, article, prompt_base)\n",
    "                for summary, article in zip(summ_eval[\"summary\"], summ_eval[\"article\"])\n",
    "            ]\n",
    "            geval_results = await asyncio.gather(*geval_tasks)\n",
    "            \n",
    "            geval_correlations[prompt_name] = summEval_correlations(summ_eval[\"consistency\"], geval_results)\n",
    "            \n",
    "            with open(CACHE_PATH, \"wb\") as f:\n",
    "                pickle.dump(CACHE, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    return geval_correlations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = {\n",
    "    \"A\": g_eval_faithfulness_prompt_a,\n",
    "    \"B\": g_eval_faithfulness_prompt_b,\n",
    "    \"C\": g_eval_faithfulness_prompt_c,\n",
    "    \"D\": g_eval_faithfulness_prompt_d\n",
    "}\n",
    "g_eval_correlations = {\n",
    "    \"phi4\": await g_eval_faithfulnesses(\"phi4\", summ_eval, prompts),\n",
    "    \"llama3.2-vision\": await g_eval_faithfulnesses(\"llama3.2-vision\", summ_eval, prompts),\n",
    "    \"deepseek-r1:14b\": await g_eval_faithfulnesses(\"deepseek-r1:14b\", summ_eval, prompts),\n",
    "    \"gpt4o-mini\": await g_eval_faithfulnesses(\"gpt4o-mini\", summ_eval, prompts),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance\n",
    "\n",
    "The performance here is actually nearly state-of-the-art. There are several methods that can do better for consistency, but this performance is high given we had no training data.\n",
    "\n",
    "That being said, all models but Phi4 had a large variation in correlation\n",
    "* Phi4 was the lowest variation, only changing by +/- 2%. This still isn't great, but likely acceptable\n",
    "* Llama3.2-vision varied by ~5% for the Spearman and Kendall-Tau scores. Which are our preferred scores due to tied rankings\n",
    "* DeepSeek-R1-Distill-Qwen-14B was all over then place. It was both the lowest performing model and had the highest variation. +/- 16%, which is unacceptable\n",
    "* GPT-4o mini had a moderate amount of variation. Still pretty high in my opinion at +/- 3-4%\n",
    "\n",
    "Other broad patterns were:\n",
    "* Prompt C was the best performing prompt for most metrics for all models. Usually followed by A, then B then D\n",
    "    * This is a little surprising because prompt D was published by Microsoft. The other 3 were hastily written and otherwise untested\n",
    "* The DeepSeek model performed the worst by far. This model is larger than the Phi4 or llama3.2-vision models, so this is also somewhat surprising\n",
    "* The other 3 models had roughly equal average performance\n",
    "    * GPT-4o mini had the highest average performance based on the Spearman and Kendall-Tau metrics, 52.2% and 47.2% respectively\n",
    "    * Llama3.2-vision had the second highest average performance, 50.0% and 46.8% respectively, but had the highest variation of the 3 performant models \n",
    "    * Phi4 was not far behind, with 49.9% and 46.2% respectively. Phi4 also had the lowest variation by far, which makes it an appealing choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2500x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "models = list(g_eval_correlations.keys())\n",
    "prompts = list(g_eval_correlations[models[0]].keys())\n",
    "correlation_names = ['Pearson', 'Spearman', 'Kendall-Tau']\n",
    "\n",
    "x = np.arange(len(models))\n",
    "width = 0.2  # width of bars\n",
    "\n",
    "fig, axes = plt.subplots(1, len(correlation_names), figsize=(25, 5), sharey=True)\n",
    "handles = []\n",
    "labels = [f'Prompt {prompt}' for prompt in prompts]\n",
    "\n",
    "for i, metric in enumerate(correlation_names):\n",
    "    ax = axes[i]\n",
    "    for j, prompt in enumerate(prompts):\n",
    "        values = [g_eval_correlations[model][prompt][i] for model in models]\n",
    "        bars = ax.bar(x + j * width, values, width)\n",
    "        \n",
    "        # Collect handles and labels only once\n",
    "        if i == 0:\n",
    "            handles.append(bars[0])\n",
    "            ax.set_ylabel(\"Score\")\n",
    "    \n",
    "    ax.set_xlabel(\"Models\")\n",
    "    ax.set_title(f\"{metric} Correlation\")\n",
    "    ax.set_xticks(x + width * 1.5)\n",
    "    ax.set_xticklabels(models)\n",
    "\n",
    "# Add a single legend to the figure\n",
    "fig.legend(handles, labels, loc='lower center', ncol=len(prompts), bbox_to_anchor=(0.5, -0.1))\n",
    "plt.suptitle(\"Evaluation Scores by Model, Technique, and Correlation Type\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent Variation\n",
      "\tphi4\n",
      "\t\tPearson +/- 1.8%\n",
      "\t\tSpearman +/- 2.0%\n",
      "\t\tKendall-Tau +/- 1.6%\n",
      "\n",
      "\tllama3.2-vision\n",
      "\t\tPearson +/- 2.4%\n",
      "\t\tSpearman +/- 5.5%\n",
      "\t\tKendall-Tau +/- 5.4%\n",
      "\n",
      "\tdeepseek-r1:14b\n",
      "\t\tPearson +/- 17.0%\n",
      "\t\tSpearman +/- 16.0%\n",
      "\t\tKendall-Tau +/- 16.1%\n",
      "\n",
      "\tgpt4o-mini\n",
      "\t\tPearson +/- 4.3%\n",
      "\t\tSpearman +/- 3.0%\n",
      "\t\tKendall-Tau +/- 3.7%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Percent Variation\")\n",
    "for model_name, correlations in g_eval_correlations.items():\n",
    "    correlations = np.array([scores for __, scores in correlations.items()])\n",
    "    pearsons, spearmans, kendall_taus = correlations[:, 0], correlations[:, 1], correlations[:, 2]\n",
    "    print(f\"\\t{model_name}\")\n",
    "    for metric_name, scores in zip([\"Pearson\", \"Spearman\", \"Kendall-Tau\"], [pearsons, spearmans, kendall_taus]):\n",
    "        percent_range = (max(scores) - min(scores)) / 2 / np.mean(scores) * 100\n",
    "        print(f\"\\t\\t{metric_name} +/- {np.round(percent_range, 1)}%\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion and Example Application\n",
    "\n",
    "Hopefully I have shown that LLMs are not a silver bullet and there are concerns in using an LLM for tasks that are not generative in nature. A task such as evaluating the faithfulness of article-summary pairs is likely best to be left to an embedding model rather than a language model.\n",
    "\n",
    "That being said, LLMs absolutely have their place in the data scientists and machine learning engineer's toolkit - even for evaluation tasks. For one, LLM are often able to very quickly produce a good enough result. Consider the following example, keeping in line with the spirit of the summary-article pairs of SummEval:\n",
    "* Let's assume we are working on an internal process that summarizes full length transcripts of video calls\n",
    "* Previously, it was company policy for employees to manually summarize meetings\n",
    "* The full transcript along with the summary is logged in some records system\n",
    "* For these summaries, there are company guidelines as far as professional language, factual correctness, etc\n",
    "* These summaries are important for legal and record keeping reasons\n",
    "* Many employees find it tedious and time consuming to write these summaries, so we want to improve this process somehow\n",
    "* It is suggested that all summaries are reviewed to ensure they follow company guidelines before being sent to the record system\n",
    "* Instead of manually reviewing every summary-transcript pair, we can use a technique like G-Eval to score them based on those existing policies\n",
    "* Any transcript-summary pair with a score of 3 or lower gets sent back to a human to be revised\n",
    "\n",
    "Better yet, we can leverage the generative nature of LLMs to create the summaries from the start\n",
    "* We have the LLM create the summary based on the full transcript\n",
    "* Feedback from the stakeholders indicates the biggest issue with the summaries is the consistency - they sometimes confuse who-said-what in the transcripts (low faithfulness - intrinsic hallucinations)\n",
    "* We use a G-Eval-based technique to evaluate the LLM-written transcript-summary pair, sending it back to the LLM to be rewritten if it scores a 3 or lower\n",
    "* After a summary with a score of 4 or 5 is created, we send it to a human for manual review some % of the time and otherwise send it straight to the record system\n",
    "\n",
    "The best part about this system was that it could be created quickly. It required no training data and the infrastructure to host the LLM already exists via Azure OpenAI Service or AWS Bedrock. The other parts of this service, like how we show it to the human for review or how we automatically log it in the record system, are necessary regardless of the machine learning technique we use to summarize and evaluate the summaries. Even if this system isn't perfect, we were able to significantly reduce the manual effort needed to write these summaries.\n",
    "\n",
    "There is a major benefit to quickly creating an imperfect system quickly: \"A lot of times, people don't know what they want until you show it to them” - Steve Jobs. Stakeholders (and people in general) often know what improvements can be made, and what would drive business value. However, they don't know how to create that system. In our example, the stakeholders knew they needed to improve the summarization, but didn't know how they could go about doing it. Create a system, even one that is imperfect like G-Eval, closes the gap between a paper diagram and the real world. After creating the system, our stakeholders can re-evaluate if we solved the problem at hand. We have a modular system, so if the summaries aren't good enough due to hallucinations, we can improve the summarization and evaluation portions of the pipeline.\n",
    "\n",
    "If we wanted to improve the evaluation step of the pipeline, MQAG [9] and TrueTeacher [10] are both techniques that are more correlated with human evaluation of consistency (faithfulness specifically, but they can also be used for coverage). MQAGs has especially high performance on SummEval data.\n",
    "* MQAG uses a seq2seq model trained on generating multiple choice questions and answers, and an embedding model is trained to answer those questions. The seq2seq model is then used to generate multiple choice questions and answers from the summary. The embedding model is trained to answers those questions based on the source article (without access to the summary). If the embedding model gets the answer incorrect, that implies information in the summary differs from information in the article (and therefore the summary is to some degree unfaithful to the article). Based on the proportion of correct answers, the overall faithfulness of the summary can be calculated.\n",
    "* TrueTeacher first generates machine written summaries from source articles. Because the summaries are machine written, a percentage are expected to have factual inconsistencies. An LLM, fine-tuned on a similar task, is then used to label each summary-article pair with a binary faithfulness label. An embedding model is then trained on the summary-article pairs with the generated labels as ground truth. The embedding model is eventually able to surpass the performance of the LLM, even though the LLM was used to generate the labels in the first place.\n",
    "\n",
    "Those methods are great for evaluating consistency, which is the biggest problem with current state-of-the-art machine written summaries. However, there are some drawbacks to those methods:\n",
    "* They can only be used to evaluate faithfulness or coverage. We would then need a different method to evaluate the professionalism of the summary. And potentially more for other aspects of the summary.\n",
    "* They require some kind of training data. TrueTeacher can be synthetically generated training data, which is a huge plus, but that still takes a lot of time and resources to develop domain specific training data.\n",
    "\n",
    "Hopefully, I have shown that G-Eval has its place and how it can be used to kickstart the development process. For some applications, it might be good enough to keep. For others, it is likely be good enough to get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Citations\n",
    "\n",
    "[1] Fabbri, Alexander R., et al. \"Summeval: Re-evaluating summarization evaluation.\" Transactions of the Association for Computational Linguistics 9 (2021): 391-409.\n",
    "\n",
    "[2] Liu, Yang, et al. \"G-eval: Nlg evaluation using gpt-4 with better human alignment.\" arXiv preprint arXiv:2303.16634 (2023).\n",
    "\n",
    "[3] Maynez, Joshua, et al. \"On faithfulness and factuality in abstractive summarization.\" arXiv preprint arXiv:2005.00661 (2020).\n",
    "\n",
    "[4] Fujimoto, K., & Maneck, B. (2024, February 24). Prompt flow. Prompt flow - Prompt flow documentation. https://microsoft.github.io/promptflow/ Maneck, B., & Fujimoto, K. (n.d.). Promptflow/examples/flows/evaluation/eval-summarization at main · Microsoft/promptflow. Summarization Evaluation. https://github.com/microsoft/promptflow/tree/main/examples/flows/evaluation/eval-summarization#meta-evaluation \n",
    "\n",
    "[5] AI@Meta. (2024). Llama 3.2 Model Card. GitHub. https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD.md \n",
    "\n",
    "[6] Abdin, Marah, et al. \"Phi-4 technical report.\" arXiv preprint arXiv:2412.08905 (2024).\n",
    "\n",
    "[7] DeepSeek-AI, et al. ‘DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning’. arXiv [Cs.CL], 2025, http://arxiv.org/abs/2501.12948. arXiv.\n",
    "\n",
    "[8] GPT-4O Mini: Advancing Cost-Efficient Intelligence. GPT-4o mini: advancing cost-efficient intelligence. (2024, July 18). https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence \n",
    "\n",
    "[9] Manakul, Potsawee, Adian Liusie, and Mark JF Gales. \"MQAG: Multiple-choice question answering and generation for assessing information consistency in summarization.\" arXiv preprint arXiv:2301.12307 (2023).\n",
    "\n",
    "[10] Gekhman, Zorik, et al. \"Trueteacher: Learning factual consistency evaluation with large language models.\" arXiv preprint arXiv:2305.11171 (2023).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
