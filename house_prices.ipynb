{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction\n",
    "\n",
    "In this notebook, we will predict housing prices on a Kaggle dataset using Gradient Boosted Regression Trees. The dataset and model was selected as it showcases a number of relevant machine learning techniques all in one project.\n",
    "\n",
    "Covered in this notebook will be:\n",
    "* Data cleaning\n",
    "* How to deal with mixed data types (ints, floats, categorical, etc)\n",
    "* Gradient Boosted Regression Trees\n",
    "    * Regression Trees\n",
    "    * Ensemble Methods\n",
    "    * Bagging\n",
    "    * Boosting\n",
    "* Grid Search\n",
    "* Cross Validation\n",
    "* Validation Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_predict\n",
    "from sklearn.metrics import root_mean_squared_error, make_scorer, r2_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This dataset was obtained from Kaggle. The output is formatted for submisssion to the Kaggle competition.\n",
    "# Anna Montoya, DataCanary. (2016). House Prices - Advanced Regression\n",
    "# Techniques. Kaggle.\n",
    "# https://kaggle.com/competitions/house-prices-advanced-regression-techniques\n",
    "\n",
    "# Set the path to inputs and outputs\n",
    "# Note: The data is hosted by Kaggle (as of 11/01/24), and you can follow the link to download it.\n",
    "#   The csv's have the same name on Kaggle as I use here\n",
    "PATH_TO_HOUSE_TRAIN = \"./data/house_prices/train.csv\"\n",
    "PATH_TO_HOUSE_TEST  = \"./data/house_prices/test.csv\"\n",
    "SAVE_DIR = \"./results/housing\"\n",
    "SAVE_VIS = False # save data visualizations\n",
    "CREATE_OUTPUT_FILE = False # save the test data to a csv for submission\n",
    "\n",
    "# Create the output path\n",
    "if SAVE_VIS or CREATE_OUTPUT_FILE:\n",
    "    os.makedirs(SAVE_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleaning\n",
    "Before processing, we need to clean the data\n",
    "* Any data with missing or infrequent entries needs to be addressed.\n",
    "* Entries that don't make sense need to be fixed.\n",
    "* Any ordinal data needs to be converted to integers (Poor, Fair, Good, Great can be 1, 2, 3, 4)\n",
    "* Any categorical data that is not of object type needs to be converted to the object type and vice versa. Ex: MSSubClass is read as an int, but that each int corresponds to a category, and those categories are not ordinal.\n",
    "* Any categorical data needs to be converted into one-hot vectors and merged with our dataset to be processed by sklearn GradientBoostingRegressor.\n",
    "\n",
    "These kinds of issues and errors are unfortunately very common in machine learning. It's common for required data to be missing or wrong, and to have to use a data wrangling tool to find and remedy the issues. Normally, I'd want to consult a SME (Subject Matter Expert) in how I clean the data - especially for missing data or where I combine similar categories.\n",
    "\n",
    "I did my best to determine what categories within a feature did and did not make sense to combine. I'm sure if I dug deeper, and especially if I spoke with an SME then I'd find more anomalies. I'm also sure I made sub-optimal choices in cleaning the data. However, this is an exercise in machine learning, not in real estate.\n",
    "\n",
    "Note: The cleaning function was written to be readable, not to be fast. This represents a first draft or proof of concept before a more optimized function would be written."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def house_data_fill_in(input_data: pd.DataFrame):\n",
    "    data = copy.deepcopy(input_data)\n",
    "\n",
    "    # We can begin by dropping columns that have too many missing values in the training data\n",
    "    # Frontage: Presumably, houses with NaN for lot frontage have 0 lot frontage. However, the documentation doesn't\n",
    "    #   confirm this, and it should be 0 if that was the case. There does not seem to be an easy relation between NaNs\n",
    "    #   and other giveaways like such as being agricultural land or an apartment which could also confirm that theory.\n",
    "    #   With ~1/6 of the data having NaN for LotFrontage, I decided to simply drop this feature for now.\n",
    "    # Utilities: Only 1 example in the training data that is not \"AllPub\". Doesn't make sense to train on 1 example\n",
    "    # Street: Similarly, there are only 5 examples that aren't \"Grvl\"\n",
    "    # RoofStyle: Several categories with too few examples, and seemingly no way to combine them well\n",
    "    # RoofMatl: 98% was one category, the other 7 categories split that 2%\n",
    "    # PoolQC: I've opted to turn the pool features into a boolean for pool and no pool\n",
    "    # Id: ID is the Id assigned by Kaggle. It isn't predictive, and if it is (shame on them), it shouldn't be used\n",
    "    data.drop([\"Id\", \"LotFrontage\", \"Utilities\", \"Street\", \"RoofStyle\", \"RoofMatl\", \"PoolQC\"], axis=1, inplace=True)\n",
    "\n",
    "    # The training and test data both had some NaN values which should be allowed to be NaN. We don't have that many\n",
    "    #   examples in the training, and we can't drop examples from the test. So for both, we will use the mode value\n",
    "    #   in the training data.\n",
    "    # The values are hard coded for readability, and so we don't have to find the mode values at test time. If this was\n",
    "    #   not a demo, the model for all columns would be in some kind of config file passed to the test preprocessing\n",
    "    data.loc[pd.isna(data[\"Functional\"]), \"Functional\"] = \"Typ\"\n",
    "    data.loc[pd.isna(data[\"KitchenQual\"]), \"KitchenQual\"] = \"TA\"\n",
    "    data.loc[pd.isna(data[\"Electrical\"]), \"Electrical\"] = \"SBrkr\"\n",
    "    data.loc[pd.isna(data[\"SaleType\"]), \"SaleType\"] = \"WD\"\n",
    "\n",
    "    # There's some entries (especially important in the test data) where 4/5 basement features indicate a basement but\n",
    "    #   one of the features is NaN. In those cases, we set the value of that feature to the average\n",
    "    basement_features1 = [\"BsmtQual\", \"BsmtCond\", \"BsmtExposure\", \"BsmtFinType1\", \"BsmtFinType2\"]\n",
    "    basement_fill_ins1 = [\"TA\",       \"TA\",       \"No\",           \"Unf\",          \"Unf\"]\n",
    "    basement_sum = data[basement_features1].isnull().sum(axis=1)\n",
    "    basement_invalid = (basement_sum != 5) & (basement_sum != 0)\n",
    "    for basement_feature, basement_fill_in in zip(basement_features1, basement_fill_ins1):\n",
    "        data.loc[basement_invalid & pd.isna(data[basement_feature]), basement_feature] = basement_fill_in\n",
    "\n",
    "    # Similarly for the garage features, either all need to be NaN or none should be NaN\n",
    "    garage_features = [\"GarageFinish\", \"GarageQual\", \"GarageCond\"]\n",
    "    garage_fill_ins = [\"Unf\",           \"TA\",        \"TA\"]\n",
    "    garage_sum = data[garage_features].isnull().sum(axis=1)\n",
    "    garage_invalid = (garage_sum != 3) & (garage_sum != 0)\n",
    "    for garage_feature, garage_fill_in in zip(garage_features, garage_fill_ins):\n",
    "        data.loc[garage_invalid & pd.isna(data[garage_feature]), garage_feature] = garage_fill_in\n",
    "    \n",
    "    # Any missing values here should be set to 0\n",
    "    other_features = [\"BsmtFinSF1\", \"BsmtFinSF2\", \"BsmtUnfSF\", \"TotalBsmtSF\", \"BsmtFullBath\", \"BsmtHalfBath\",\n",
    "                      \"GarageCars\", \"GarageArea\"]\n",
    "    for other_features in other_features:\n",
    "        data.loc[pd.isna(data[other_features]), other_features] = 0.0\n",
    "\n",
    "    # For the following features, there were several categories that had too few values to use for training. The concern\n",
    "    #   is that the model will overfit on the relatively few examples. There's no hard cutoff. I tried to group similar\n",
    "    #   categories together if one category had less than ~20 items. However, there were exceptions where there was not\n",
    "    #   a similar category to combine into\n",
    "\n",
    "    # MSSubClass: Combine 20 (1-Story 1946 & Newer) and 40 (1 Story W/finished Attic All Ages)\n",
    "    #   Combine 45 (1-1/2 Story Unfinished) and 50 (1-1/2 Story Finished)\n",
    "    #   Combine 75 (2-1/2 Story All Ages), 80 (Split or Multi-Level), and 85 (Split Foyer)\n",
    "    #   Combine 120 (1-Story PUD (Planned Unit Development)) and 150 (1-1/2 Story PUD)\n",
    "    #   Combine 160 (2-Story PUD) and 180 (Multi-Level PUD) \n",
    "    data.loc[data[\"MSSubClass\"] == 40, \"MSSubClass\"] = 20\n",
    "    data.loc[data[\"MSSubClass\"] == 45, \"MSSubClass\"] = 50\n",
    "    data.loc[data[\"MSSubClass\"] == 75, \"MSSubClass\"] = 80\n",
    "    data.loc[data[\"MSSubClass\"] == 85, \"MSSubClass\"] = 80\n",
    "    data.loc[data[\"MSSubClass\"] == 120, \"MSSubClass\"] = 150\n",
    "    data.loc[data[\"MSSubClass\"] == 160, \"MSSubClass\"] = 180\n",
    "    \n",
    "    # MSZoning: I (Industrial), C (Commercial), and A (Agricultural) can all be combined into one category: Other\n",
    "    # RH (Residential  High Density) is close to RM (Residential Medium Density)\n",
    "    # RP (Residential Low Density Park) is close to RL (Residential Low Density)\n",
    "    data.loc[data[\"MSZoning\"] == \"A\", \"MSZoning\"] = \"Other\"\n",
    "    data.loc[data[\"MSZoning\"] == \"C\", \"MSZoning\"] = \"Other\"\n",
    "    data.loc[data[\"MSZoning\"] == \"I\", \"MSZoning\"] = \"Other\"\n",
    "    data.loc[data[\"MSZoning\"] == \"C (all)\", \"MSZoning\"] = \"Other\"\n",
    "    data.loc[data[\"MSZoning\"] == \"RH\", \"MSZoning\"] = \"RM\"\n",
    "    data.loc[data[\"MSZoning\"] == \"RP\", \"MSZoning\"] = \"RL\"\n",
    "\n",
    "    # LotShape: IR3 (Irregular) is close to IR2 (Moderately Irregular)\n",
    "    data.loc[data[\"LotShape\"] == \"IR3\", \"LotShape\"] = \"IR2\" \n",
    "    \n",
    "    # LotConfig: FR3 (Frontage on 3 sides of property) is close to FR2 (Frontage on 2 sides of property)\n",
    "    data.loc[data[\"LotConfig\"] == \"FR3\", \"LotConfig\"] = \"FR2\" \n",
    "\n",
    "    # LandSlope: Sev (Severe) is close to Mod (Moderate)\n",
    "    data.loc[data[\"LandSlope\"] == \"Sev\", \"LandSlope\"] = \"Mod\" \n",
    "\n",
    "    # Neighborhood: There are several neighborhoods with too few examples. Ideally, I'd combine them with a similar\n",
    "    #   neighborhood using distance between the neighborhoods and sale price as a heuristic. But for this short\n",
    "    #   demonstration, I'm just going to set all those neighborhoods to \"Other\"\n",
    "    data.loc[data[\"Neighborhood\"] == \"Blueste\", \"Neighborhood\"] = \"Other\" \n",
    "    data.loc[data[\"Neighborhood\"] == \"NPkVill\", \"Neighborhood\"] = \"Other\" \n",
    "    data.loc[data[\"Neighborhood\"] == \"Veenker\", \"Neighborhood\"] = \"Other\" \n",
    "    data.loc[data[\"Neighborhood\"] == \"BrDale\", \"Neighborhood\"] = \"Other\" \n",
    "    data.loc[data[\"Neighborhood\"] == \"Blmngtn\", \"Neighborhood\"] = \"Other\" \n",
    "    data.loc[data[\"Neighborhood\"] == \"MeadowV\", \"Neighborhood\"] = \"Other\" \n",
    "\n",
    "    # Condition1: Combine PosA (Adjacent to positive off-site feature) with PosN (Near positive off-site feature)\n",
    "    # Combine All Near/Adjacent to a Railroad\n",
    "    data.loc[data[\"Condition1\"] == \"PosA\", \"Condition1\"] = \"PosN\" \n",
    "    data.loc[data[\"Condition1\"] == \"RRNe\", \"Condition1\"] = \"RR\" \n",
    "    data.loc[data[\"Condition1\"] == \"RRNn\", \"Condition1\"] = \"RR\" \n",
    "    data.loc[data[\"Condition1\"] == \"RRAn\", \"Condition1\"] = \"RR\" \n",
    "    data.loc[data[\"Condition1\"] == \"RRNe\", \"Condition1\"] = \"RR\" \n",
    "    data.loc[data[\"Condition1\"] == \"RRAe\", \"Condition1\"] = \"RR\" \n",
    "\n",
    "    # Condition2: Combine PosA (Adjacent to positive off-site feature) with PosN (Near positive off-site feature)\n",
    "    # Combine All Near/Adjacent to a Railroad\n",
    "    data.loc[data[\"Condition2\"] == \"PosA\", \"Condition1\"] = \"PosN\" \n",
    "    data.loc[data[\"Condition2\"] == \"RRNe\", \"Condition2\"] = \"RR\" \n",
    "    data.loc[data[\"Condition2\"] == \"RRNn\", \"Condition2\"] = \"RR\" \n",
    "    data.loc[data[\"Condition2\"] == \"RRAn\", \"Condition2\"] = \"RR\" \n",
    "    data.loc[data[\"Condition2\"] == \"RRNe\", \"Condition2\"] = \"RR\" \n",
    "    data.loc[data[\"Condition2\"] == \"RRAe\", \"Condition2\"] = \"RR\" \n",
    "\n",
    "    # HouseStyle: Combine 2.5Fin and 2.5Unf (2.5 Story Finished/Unfinished)\n",
    "    #   Combine 1.5Fin and 1.5Unf (1.5 Story Finished/Unfinished)\n",
    "    data.loc[data[\"HouseStyle\"] == \"2.5Fin\", \"HouseStyle\"] = \"2.5\" \n",
    "    data.loc[data[\"HouseStyle\"] == \"2.5Unf\", \"HouseStyle\"] = \"2.5\" \n",
    "    data.loc[data[\"HouseStyle\"] == \"1.5Fin\", \"HouseStyle\"] = \"1.5\" \n",
    "    data.loc[data[\"HouseStyle\"] == \"1.5Unf\", \"HouseStyle\"] = \"1.5\" \n",
    "\n",
    "    # OverallQual: Combine 1, 2, 3 (Too few 1's and 2's). Combine 9 and 10 (Too few 10's)\n",
    "    #   Combine 1.5Fin and 1.5Unf (1.5 Story Finished/Unfinished)\n",
    "    data.loc[data[\"OverallQual\"] == 1, \"OverallQual\"] = 3\n",
    "    data.loc[data[\"OverallQual\"] == 2, \"OverallQual\"] = 3\n",
    "    data.loc[data[\"OverallQual\"] == 10, \"OverallQual\"] = 9\n",
    "\n",
    "    # OverallCond: Combine 1, 2, 3 (Too few 1's and 2's). Combine 9 and 10 (No 10's)\n",
    "    #   Combine 1.5Fin and 1.5Unf (1.5 Story Finished/Unfinished)\n",
    "    data.loc[data[\"OverallCond\"] == 1, \"OverallCond\"] = 3\n",
    "    data.loc[data[\"OverallCond\"] == 2, \"OverallCond\"] = 3\n",
    "    data.loc[data[\"OverallCond\"] == 10, \"OverallCond\"] = 9\n",
    "\n",
    "    # Exterior1st/Exterior2nd: Combine ImStucc (immitation stucco) and Stucco\n",
    "    # Mark lesser occuring types as \"Other\"\n",
    "    data.loc[data[\"Exterior1st\"] == \"ImStucc\", \"Exterior1st\"] = \"Stucco\"\n",
    "    data.loc[data[\"Exterior1st\"] == \"AsphShn\", \"Exterior1st\"] = \"Other\"\n",
    "    data.loc[data[\"Exterior1st\"] == \"CBlock\", \"Exterior1st\"] = \"Other\"\n",
    "    data.loc[data[\"Exterior1st\"] == \"Stone\", \"Exterior1st\"] = \"Other\"\n",
    "    data.loc[data[\"Exterior1st\"] == \"BrkComm\", \"Exterior1st\"] = \"Other\"\n",
    "    data.loc[data[\"Exterior1st\"] == \"Brk Cmn\", \"Exterior1st\"] = \"Other\"\n",
    "    \n",
    "    data.loc[data[\"Exterior2nd\"] == \"ImStucc\", \"Exterior2nd\"] = \"Stucco\"\n",
    "    data.loc[data[\"Exterior2nd\"] == \"AsphShn\", \"Exterior2nd\"] = \"Other\"\n",
    "    data.loc[data[\"Exterior2nd\"] == \"CBlock\", \"Exterior2nd\"] = \"Other\"\n",
    "    data.loc[data[\"Exterior2nd\"] == \"Stone\", \"Exterior2nd\"] = \"Other\"\n",
    "    data.loc[data[\"Exterior2nd\"] == \"BrkComm\", \"Exterior2nd\"] = \"Other\"\n",
    "    data.loc[data[\"Exterior2nd\"] == \"Brk Cmn\", \"Exterior2nd\"] = \"Other\"\n",
    "    \n",
    "    # Exterior1st/Exterior2nd: Combine ImStucc (immitation stucco) and Stucco\n",
    "    # Mark lesser occuring types as \"Other\"\n",
    "    data.loc[data[\"Exterior1st\"] == \"ImStucc\", \"Exterior1st\"] = \"Stucco\"\n",
    "    data.loc[data[\"Exterior1st\"] == \"AsphShn\", \"Exterior1st\"] = \"Other\"\n",
    "    data.loc[data[\"Exterior1st\"] == \"CBlock\", \"Exterior1st\"] = \"Other\"\n",
    "    data.loc[data[\"Exterior1st\"] == \"Stone\", \"Exterior1st\"] = \"Other\"\n",
    "    data.loc[data[\"Exterior1st\"] == \"BrkComm\", \"Exterior1st\"] = \"Other\"\n",
    "    data.loc[data[\"Exterior1st\"] == \"Brk Cmn\", \"Exterior1st\"] = \"Other\"\n",
    "    \n",
    "    data.loc[data[\"Exterior2nd\"] == \"ImStucc\", \"Exterior2nd\"] = \"Stucco\"\n",
    "    data.loc[data[\"Exterior2nd\"] == \"AsphShn\", \"Exterior2nd\"] = \"Other\"\n",
    "    data.loc[data[\"Exterior2nd\"] == \"CBlock\", \"Exterior2nd\"] = \"Other\"\n",
    "    data.loc[data[\"Exterior2nd\"] == \"Stone\", \"Exterior2nd\"] = \"Other\"\n",
    "    data.loc[data[\"Exterior2nd\"] == \"BrkComm\", \"Exterior2nd\"] = \"Other\"\n",
    "    data.loc[data[\"Exterior2nd\"] == \"Brk Cmn\", \"Exterior2nd\"] = \"Other\"\n",
    "    \n",
    "    # ExterQual: Fa (Fair) and Po (Poor) combine with TA (Average/Typical)\n",
    "    data.loc[data[\"BsmtCond\"] == \"Ex\", \"BsmtCond\"] = \"Gd\"\n",
    "    data.loc[data[\"BsmtCond\"] == \"Po\", \"BsmtCond\"] = \"Fa\"\n",
    "    data.loc[data[\"BsmtCond\"] == \"Wood\", \"BsmtCond\"] = \"Other\"\n",
    "\n",
    "    # Heating: Combine everything other than GasA (Gas forced warm air furnace)\n",
    "    data.loc[data[\"Heating\"] != \"GasA\", \"Heating\"] = \"Other\"\n",
    "    \n",
    "    # HeatingQC: Combine Po (Poor) and Fa (Fair)\n",
    "    data.loc[data[\"HeatingQC\"] == \"Po\", \"HeatingQC\"] = \"Fa\"\n",
    "\n",
    "    # Electrical: Combine FuseP and FuseF\n",
    "    # The 1 mixed example can be set to the mode (SBrkr)\n",
    "    data.loc[data[\"Electrical\"] == \"FuseP\", \"Electrical\"] = \"FuseF\"\n",
    "    data.loc[data[\"Electrical\"] == \"Mix\", \"Electrical\"] = \"SBrkr\"\n",
    "\n",
    "    # KitchenQual: Combine Po (Poor) and Fa (Fair)\n",
    "    data.loc[data[\"KitchenQual\"] == \"Po\", \"KitchenQual\"] = \"Po\"\n",
    "\n",
    "    # Functional: Combine Sev (Severe), Maj2 (Major Deductions 2) and Maj1 (Major Deductions 1)\n",
    "    data.loc[data[\"Functional\"] == \"Sev\", \"Functional\"] = \"Maj1\"\n",
    "    data.loc[data[\"Functional\"] == \"Maj2\", \"Functional\"] = \"Maj1\"\n",
    "\n",
    "    # GarageType: Combine CarPort and Detchd (Detached)\n",
    "    # Combine 2Types and Attchd (Attached) - 2Types likely means attached + another\n",
    "    data.loc[data[\"GarageType\"] == \"CarPort\", \"GarageType\"] = \"Detchd\"\n",
    "    data.loc[data[\"GarageType\"] == \"2Types\", \"GarageType\"] = \"Attchd\"\n",
    "\n",
    "    # GarageYrBlt: If there is no garage, set the garage year built to the house built year\n",
    "    garage_invalid = pd.isna(data[\"GarageYrBlt\"])\n",
    "    data.loc[garage_invalid, \"GarageYrBlt\"] = data[\"YearBuilt\"][garage_invalid]\n",
    "    \n",
    "    # GarageQual: Combine Po (Poor) and Fa (Fair). Combine Ex (Excellent) and Gd (Good)\n",
    "    data.loc[data[\"GarageQual\"] == \"Po\", \"GarageQual\"] = \"Fa\"\n",
    "    data.loc[data[\"GarageQual\"] == \"Ex\", \"GarageQual\"] = \"Gd\"\n",
    "\n",
    "    # GarageCond: Combine Po (Poor) and Fa (Fair). Combine Ex (Excellent), Gd (Good), and TA (Typical/Average)\n",
    "    data.loc[data[\"GarageCond\"] == \"Po\", \"GarageCond\"] = \"Fa\"\n",
    "    data.loc[data[\"GarageCond\"] == \"Ex\", \"GarageCond\"] = \"TA\"\n",
    "    data.loc[data[\"GarageCond\"] == \"Gd\", \"GarageCond\"] = \"TA\"\n",
    "\n",
    "    # Fence: Combine MnWw (Minimum Wood/Wire) with GdWo (Good Wood)\n",
    "    data.loc[data[\"Fence\"] == \"MnWw\", \"Fence\"] = \"GdWo\"\n",
    "\n",
    "    # MiscFeature: Drop all non-shed options\n",
    "    data.loc[data[\"MiscFeature\"] != \"Shed\", \"MiscFeature\"] = \"Na\"\n",
    "\n",
    "    # SaleType: Combine all conditional (Con, ConLw, ConLI, ConLD) with Oth (Other)\n",
    "    data.loc[data[\"SaleType\"] == \"ConLw\", \"SaleType\"] = \"Oth\"\n",
    "    data.loc[data[\"SaleType\"] == \"ConLI\", \"SaleType\"] = \"Oth\"\n",
    "    data.loc[data[\"SaleType\"] == \"ConLD\", \"SaleType\"] = \"Oth\"\n",
    "    data.loc[data[\"SaleType\"] == \"Con\", \"SaleType\"] = \"Oth\"\n",
    "    data.loc[data[\"SaleType\"] == \"CWD\", \"SaleType\"] = \"WD\"\n",
    "\n",
    "    # SaleCondition: Combine AdjLand with Alloca\n",
    "    data.loc[data[\"SaleCondition\"] == \"AdjLand\", \"SaleCondition\"] = \"Alloca\"\n",
    "\n",
    "    # In this next step, I assume that for these data types, \"nan\" means none or not applicable as per the data\n",
    "    #   description file. It is possible that these are actually unknown, which is bad data practice on the part of the\n",
    "    #   data aggregator.\n",
    "    nan_features = [\"Alley\", \"BsmtQual\", \"BsmtCond\", \"BsmtExposure\", \"BsmtFinType1\", \"BsmtFinType2\", \"FireplaceQu\",\n",
    "                    \"GarageType\", \"GarageFinish\", \"GarageQual\", \"GarageCond\", \"Fence\",]\n",
    "    for col in nan_features:\n",
    "        data[col] = np.where(pd.isnull(data[col]), \"Na\", data[col])\n",
    "\n",
    "    # How can a house have no masonry veneer (MasVnrType is category NA), but have\n",
    "    #   a non-zero masonry veneer area (MasVnrArea > 0). There's only 5 entries\n",
    "    #   where this a problem.\n",
    "    data[\"MasVnrArea\"] = np.where(\n",
    "        pd.isnull(data[\"MasVnrType\"]), 0.0, data[\"MasVnrArea\"]\n",
    "    )\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def house_data_convert_types(input_data: pd.DataFrame):\n",
    "    data = copy.deepcopy(input_data)\n",
    "\n",
    "    data[\"MSSubClass\"] = data[\"MSSubClass\"].astype(str)\n",
    "    data[\"MasVnrArea\"] = data[\"MasVnrArea\"].astype(int) \n",
    "    data[\"GarageYrBlt\"] = data[\"GarageYrBlt\"].astype(int) \n",
    "\n",
    "    data[\"LotShape\"].replace({\"Reg\": 0, \"IR1\": 1, \"IR2\": 2}, inplace=True)\n",
    "    data[\"LotShape\"].astype(int)\n",
    "\n",
    "    data[\"LandContour\"].replace({\"Low\":-1, \"Lvl\":0, \"Bnk\":1, \"HLS\":2}, inplace=True)\n",
    "    data[\"LandContour\"].astype(int)\n",
    "\n",
    "    data[\"LandSlope\"].replace({\"Gtl\":0, \"Mod\":1, \"Sev\":2}, inplace=True)\n",
    "    data[\"LandSlope\"].astype(int)\n",
    "\n",
    "    data[\"BsmtExposure\"].replace({\"Na\":-1, \"No\":0, \"Mn\":1, \"Av\":2, \"Gd\":3}, inplace=True)\n",
    "    data[\"BsmtExposure\"].astype(int)\n",
    "\n",
    "    data[\"BsmtFinType1\"].replace({\"Na\":-1, \"Unf\":0, \"LwQ\":1, \"Rec\":2, \"BLQ\":3, \"ALQ\":4, \"GLQ\":5}, inplace=True)\n",
    "    data[\"BsmtFinType1\"].astype(int)\n",
    "\n",
    "    data[\"BsmtFinType2\"].replace({\"Na\":-1, \"Unf\":0, \"LwQ\":1, \"Rec\":2, \"BLQ\":3, \"ALQ\":4, \"GLQ\":5}, inplace=True)\n",
    "    data[\"BsmtFinType2\"].astype(int)\n",
    "\n",
    "    data[\"Functional\"].replace({\"Sal\":0, \"Sev\":1, \"Maj2\":2, \"Maj1\":3, \"Mod\":4, \"Min2\":5, \"Min1\":6, \"Typ\":7}, inplace=True)\n",
    "    data[\"Functional\"].astype(int)\n",
    "\n",
    "    data[\"GarageFinish\"].replace({\"Na\":0, \"Unf\":1, \"RFn\":2, \"Fin\":3}, inplace=True)\n",
    "    data[\"GarageFinish\"].astype(int)\n",
    "\n",
    "    # Convert pool area to boolean of pool and no pool\n",
    "    data[\"Pool\"] = data[\"PoolArea\"] > 0\n",
    "    data.drop(\"PoolArea\", axis=1, inplace=True)\n",
    "\n",
    "    # Convert sold int Year/Mo to float\n",
    "    data[\"YrSold\"] = data[\"YrSold\"].astype(float)\n",
    "    data[\"YrSold\"] += data[\"MoSold\"] / 12\n",
    "    data.drop(\"MoSold\", axis=1, inplace=True)\n",
    "\n",
    "    # Features that only have 2 options can be converted to a boolean rather than a one-hot vector\n",
    "    data[\"Heating\"].replace({\"Other\":0, \"GasA\":1}, inplace=True)\n",
    "    data[\"Heating\"].astype(int)\n",
    "    \n",
    "    data[\"CentralAir\"].replace({\"N\":0, \"Y\":1}, inplace=True)\n",
    "    data[\"CentralAir\"].astype(int)\n",
    "\n",
    "    data[\"MiscFeature\"].replace({\"Na\":0, \"Shed\":1}, inplace=True)\n",
    "    data[\"MiscFeature\"].astype(int)\n",
    "\n",
    "    # Replace all Poor/Fair/Good/etc ratings\n",
    "    rating_features = [\"ExterQual\", \"ExterCond\", \"BsmtQual\", \"BsmtCond\", \"HeatingQC\", \"KitchenQual\", \"FireplaceQu\",\n",
    "                       \"GarageQual\", \"GarageCond\"]\n",
    "    for rating_feature in rating_features:\n",
    "        data[rating_feature].replace({\"Na\":0, \"Po\":1, \"Fa\":2, \"TA\":3, \"Gd\":4, \"Ex\":5}, inplace=True)\n",
    "        data[rating_feature].astype(int)\n",
    "\n",
    "    # These are a typo, but the type needs to be fixed to later combine them\n",
    "    data.loc[data[\"Exterior1st\"] == \"CemntBd\", \"Exterior1st\"] = \"CmentBd\"\n",
    "    data.loc[data[\"Exterior2nd\"] == \"Wd Shng\", \"Exterior2nd\"] = \"WdShing\"\n",
    "\n",
    "    data = pd.get_dummies(\n",
    "        data,\n",
    "        columns=data.select_dtypes(include=[\"object\"]).columns,\n",
    "        dtype='bool'\n",
    "    )\n",
    "\n",
    "    # Combine the Condition1 and Condition2 booleans\n",
    "    #   There are some cases where cond1 or cond2 is missing an item, so we have to use some special logic to get it\n",
    "    cond_new = pd.Series(data=False, index=data.index, name='condition')\n",
    "    for name in [\"Artery\", \"Feedr\", \"Norm\", \"PosN\", \"RR\"]:\n",
    "        cond1 = \"Condition1_\" + name\n",
    "        cond2 = \"Condition2_\" + name\n",
    "        if cond1 not in data and cond2 not in data:\n",
    "            continue\n",
    "\n",
    "        data[\"Condition_\" + name] = data.get(cond1, cond_new) + data.get(cond2, cond_new)\n",
    "        if cond1 in data:\n",
    "            data.drop([cond1], axis=1, inplace=True)\n",
    "        if cond2 in data:\n",
    "            data.drop([cond2], axis=1, inplace=True)\n",
    "\n",
    "    # Combine the Exterior1 and Exterior2 booleans\n",
    "    for name in [\"AsbShng\", \"BrkFace\", \"CmentBd\", \"HdBoard\", \"MetalSd\", \"Other\", \"Plywood\", \"Stucco\", \"VinylSd\", \"Wd Sdng\", \"WdShing\"]:\n",
    "        data[\"Exterior_\" + name] = data[\"Exterior1st_\" + name] + data[\"Exterior2nd_\" + name]\n",
    "        data.drop([\"Exterior1st_\" + name, \"Exterior2nd_\" + name], axis=1, inplace=True)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the training data\n",
    "training = pd.DataFrame(pd.read_csv(PATH_TO_HOUSE_TRAIN))\n",
    "\n",
    "# Clean the training data\n",
    "training = house_data_fill_in(training)\n",
    "training = house_data_convert_types(training)\n",
    "\n",
    "# Split the data into features and labels\n",
    "training_prices = training['SalePrice']\n",
    "training.drop(['SalePrice'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the Model - Gradient Boosted Regression Trees\n",
    "\n",
    "This dataset has several types of features (ints, float, booleans) which limits the number of models that will perform well on the data (without feature engineering). Some variant of decision trees are often used in these kinds of cases, which Gradient Boosted Regression Trees are a variant on Decision Trees.\n",
    "\n",
    "This model uses an ensemble of regression trees to fit the data. I use a grid search over the parameter space, bagging during training to reduce overfitting, and cross validation to reduce overfitting as well as for preliminary evaluation.\n",
    "* Regression Trees - The feature set of our data is a combination of integers, floats, and categorical data (categorical data represented as on-hot vector of booleans). A regression tree is just a decision tree that outputs a value (house price prediction) rather than a classification.\n",
    "* Ensemble Method - The \"Gradient Boosted\" part. Instead of building a single very large tree (strong classifier), we build many small trees (weak classifiers) and feed the results of one tree into the next to get better performance. Each iteration tries to correct the error from the previous iteration.\n",
    "* Bagging - Each weak classifier is trained on a subset of the training data. This significantly speeds up the training process (50% of the data means 2x as fast) while reducing overfitting.\n",
    "* Grid Search - A simple grid search over the parameter space, keeping the parameters that score the best on the validation data.\n",
    "* Cross Validation - Cross validation helps ensure that we are not overfitting and gives us insight into how the model will perform on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the parameter search space\n",
    "# Note: I searched ~3x above and below this search space while trialing. I am working on my own home computer and\n",
    "#   didn't want it running for terribly long either. So I kept the search space relatively small.\n",
    "param_grid = {\n",
    "    \"learning_rate\": (0.005, 0.008, 0.012),\n",
    "    \"n_estimators\": (1500, 3000, 4000),\n",
    "    \"max_depth\": (2, 3, 4),\n",
    "    \"min_samples_split\": (2, 3, 4),\n",
    "}\n",
    "\n",
    "# We want to minimuze the error (error metric defined by Kaggle). GridSearchCV will find the set of parameters with the\n",
    "#   maximum score in the search space, so we use the negative of the Kaggle error function as our scoring function.\n",
    "score_func = make_scorer(\n",
    "    lambda x, y : -root_mean_squared_error(np.log(x), np.log(y))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Params: {'learning_rate': 0.008, 'max_depth': 3, 'min_samples_split': 3, 'n_estimators': 4000}\n",
      "Kaggle Error: 0.11888067073000833\n"
     ]
    }
   ],
   "source": [
    "# Perform the grid search over the given parameters with 5-fold cross validation.\n",
    "# The subsample rate is set to 0.5, which adds bagging to the training.\n",
    "#   Varying the subsample rate between 0.5 - 1.0 did not seem to change performance by much, but did reduce training\n",
    "#   time since the trees only train on half the data. In general this should reduce model variance.\n",
    "gs = GridSearchCV(\n",
    "    GradientBoostingRegressor(loss=\"squared_error\", subsample=0.5),\n",
    "    param_grid,\n",
    "    scoring=score_func,\n",
    "    cv = 5,\n",
    "    n_jobs = 4,\n",
    "    pre_dispatch='n_jobs',\n",
    "    refit=True\n",
    ")\n",
    "gs.fit(training, training_prices)\n",
    "print(\"Best Params:\", gs.best_params_)\n",
    "\n",
    "# Get the Kaggle score. From Kaggle: \"Submissions are evaluated on Root-Mean-Squared-Error (RMSE) between the logarithm\n",
    "#   of the predicted value and the logarithm of the observed sales price.\"\n",
    "# This is done so the error is relative, and more equally weights the error of cheap and expensive houses.\n",
    "# Notably though, this metric also incurs a higher penalty for underestimation than overestimation. This benefits the\n",
    "#   seller since our model will likely slightly overestimate the value of a house rather than underestimate it. If this\n",
    "#   was for a real-world application, I'd chat with stakeholders to see if that is a desired feature.\n",
    "print(\"Kaggle Error:\", -gs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To evaluate, we can get the average performance over a cross validation\n",
    "best_estimator = gs.best_estimator_\n",
    "training_pred = cross_val_predict(\n",
    "    GradientBoostingRegressor(**gs.best_params_),\n",
    "    training,\n",
    "    training_prices,\n",
    "    cv=5,\n",
    "    n_jobs=1\n",
    ")\n",
    "\n",
    "# A more intuitive meaningful metric than the RMSE of log difference is correlation.\n",
    "# The Pearson coefficient of ~90%, which is great performance for a small model with no hand-crafted features.\n",
    "# That translates to ~90% of the sale price variance can be explained by this model.\n",
    "print(\"Validation Coefficient of Determination:\", r2_score(training_prices, training_pred))\n",
    "\n",
    "# Plot the true vs predicted sale price on the validation data\n",
    "plt.figure(\"true_vs_pred\")\n",
    "plt.scatter(training_prices/1000, training_pred/1000, alpha=0.2, linewidths=0)\n",
    "plt.xlabel(\"True House Price ($1,000)\")\n",
    "plt.ylabel(\"Predicted House Price ($1,000)\")\n",
    "one2one = np.array([min(min(training_prices), min(training_pred)),\n",
    "                    max(max(training_prices), max(training_pred))])/1000\n",
    "plt.plot(one2one, one2one, c='g', linestyle=\"--\")\n",
    "plt.xlim(left=0)\n",
    "plt.ylim(bottom=0)\n",
    "plt.show()\n",
    "if SAVE_VIS:\n",
    "    plt.savefig(\n",
    "        os.path.join(SAVE_DIR, \"house_price_validation_performance.png\"),\n",
    "        bbox_inches=\"tight\"\n",
    "    )\n",
    "plt.close(\"true_vs_pred\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the test data, we can't remove entries. Only modified the data similarly to how we did for the input data. For\n",
    "#   entries that can't be removed, we can set them to the average, or the most common value\n",
    "test = pd.DataFrame(pd.read_csv(PATH_TO_HOUSE_TEST))\n",
    "test = house_data_fill_in(test)\n",
    "test = house_data_convert_types(test)\n",
    "\n",
    "# Get the prices of the testing data houses\n",
    "test_prices = best_estimator.predict(test)\n",
    "\n",
    "# Output the results in the format accepted by Kaggle\n",
    "output = pd.DataFrame({\"Id\": range(1461, 2920), \"SalePrice\": test_prices})\n",
    "if CREATE_OUTPUT_FILE:\n",
    "    output.to_csv(\n",
    "        os.path.join(SAVE_DIR, \"house_price_submission.csv\"),\n",
    "        index=False\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
