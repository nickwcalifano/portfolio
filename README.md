# Nicholas Califano
### ML/AI Portfolio

This portfolio is comprised of examples projects intended to highlight a variety of machine learning techniques and simulate a number of discussions with stakeholders. The projects are intentionally written to be a demonstration and discussion (almost like a presentation) rather than an optimized application.

The performance of these methods is often acceptable but not spectacular due to intentionally using small datasets, small models, and small compute resources (my personal computer). The goal of this portfolio is to show competency with these techniques, not state-of-the-art results. 

### Examples
| Example          | Dataset                                     | Demonstration of:                                   | Results                        | Simulated Discussions with Stakeholders   | Potential Future Work          |
|------------------|---------------------------------------------|-----------------------------------------------------|--------------------------------|-------------------------------------------|--------------------------------|
| llm_as_a_judge.ipynb        | [SummEval](https://github.com/nlpyang/geval/blob/main/data/summeval.json) | Relevant Libraries: Ollama, Azure OpenAI Service, aiohttp, asyncio <br /><br /> Models: Large Language Models (Llama3.2-vision, Phi4, DeepSeek-R1-Distill-Qwen-14B, and GPT-4o mini) <br /><br /> ML Techniques: Prompt Engineering, Sensitivity Analysis <br /><br />  Other: LLM-as-a--judge, Text Evaluation | Up to ~60% Correlation with Human Evaluations | - The 'best' method (accuracy, monetary/compute cost) isnâ€™t always the best at generating business value. <br /> - Building a minimum viable product quickly, then quickly iterating (Agile) enables more feedback loops and better alignment between technical staff and stakeholders. | - More Performative LLMs (GPT-4o, o1-mini, o1) <br /> - Advanced Prompt Engineering (CoT, Few-Shot, etc) |
| multiclass_image_classification.ipynb        | [CIFAR10](https://www.cs.toronto.edu/~kriz/cifar.html) | Relevant Libraries: Tensorflow, Keras <br /><br /> Models: Convolutional Neural Networks, Pretrained ResNet18 <br /><br /> ML Techniques: Data Augmentation, Regularization, Transfer Learning <br /><br />  Other: Image Classification, Supervised Learning | Up to ~85% Accuracy, 10 Categories, on Test Data | - Do we have a similar dataset available for pretraining? | - Pretrain Custom Model on a Similar Dataset (CIFAR100 minus CIFAR10 classes) <br /> - Experiment with more Data Augmentation <br /> - Experiment with better Learning Rate Scheduling  |
| regression_analysis.ipynb | [Quora Insincere Questions](https://www.kaggle.com/c/quora-insincere-questions-classification/overview) | Relevant Libraries: transformers, PyTorch <br /><br /> Model: RoBERTa  <br /><br /> ML Techniques: Pretrained Embedding Model, Transformers, Regularization, Class Imbalance Mitigations <br /><br /> Other: Text Classification, Supervised Learning | TODO | - Automatic flagging of content in a production environment <br /> - Trade offs between precision and recall <br /> - Using machine learning to allocate resources  | - Using nlpaug and 1 order of magnitude less training data <br /> - Using other techniques and 2 orders of magnitude less training data | 
